{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa5e9b8-ed16-435b-8371-997bf5dd5750",
   "metadata": {},
   "source": [
    "# ResNet18 finetuning convergence test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac118d15",
   "metadata": {},
   "source": [
    "## Prerequisite: start the cluster\n",
    "\n",
    "To use this notebook, we need an A100 VM and a TPU v5litepod-8 VM.\n",
    "Both VMs should be reachable via IP address to each other.\n",
    "This notebook is run on the A100 VM, which is also the ray head node.\n",
    "\n",
    "Starting the head node:\n",
    "\n",
    "```sh\n",
    "ray start --head --port=6379\n",
    "```\n",
    "\n",
    "Joining the ray cluster from the TPU VM:\n",
    "\n",
    "```sh\n",
    "ray start --address=$GPU_IP:6379\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21762670",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 19:24:56,999\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.128.0.32:6379...\n",
      "2025-04-30 19:24:57,043\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d5e7ae437641ed9a0b94522cade960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.10.14</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.44.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.10.14', ray_version='2.44.1', ray_commit='daca7b2b1a950dc7f731e34e74c76ae383794ffe')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/30 12:26:15\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.076860 1711130 b295d63588a.cc:733] Linux version 6.5.0-1013-gcp (buildd@lcy02-amd64-064) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #13~22.04.1-Ubuntu SMP Wed Jan 24 23:39:40 UTC 2024\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087251 1711130 b295d63588a.cc:815] Process id 1711130\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087266 1711130 b295d63588a.cc:820] Current working directory /home/yifeit_google_com/resnet18\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087267 1711130 b295d63588a.cc:822] Current timezone is PDT (currently UTC -07:00)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087270 1711130 b295d63588a.cc:826] Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087271 1711130 b295d63588a.cc:827]  at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087272 1711130 b295d63588a.cc:828]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087273 1711130 b295d63588a.cc:829]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087274 1711130 b295d63588a.cc:832]  from changelist 719471581 with baseline 706826864 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/706826864.1/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087275 1711130 b295d63588a.cc:836] Build label: libtpu_lts_20241216_c_RC02\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087276 1711130 b295d63588a.cc:838] Build tool: Bazel, release r4rca-2024.12.08-1 (mainline @703883733)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087277 1711130 b295d63588a.cc:839] Build target: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087278 1711130 b295d63588a.cc:846] Command line arguments:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087279 1711130 b295d63588a.cc:848] argv[0]: './tpu_driver'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087281 1711130 b295d63588a.cc:848] argv[1]: '--minloglevel=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087283 1711130 b295d63588a.cc:848] argv[2]: '--stderrthreshold=3'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087284 1711130 b295d63588a.cc:848] argv[3]: '--v=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087286 1711130 b295d63588a.cc:848] argv[4]: '--vmodule='\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087287 1711130 b295d63588a.cc:848] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087288 1711130 b295d63588a.cc:848] argv[6]: '--max_log_size=1024'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087289 1711130 b295d63588a.cc:848] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087291 1711130 b295d63588a.cc:848] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087292 1711130 b295d63588a.cc:848] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087293 1711130 b295d63588a.cc:848] argv[10]: '--2a886c8_chips_per_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087295 1711130 b295d63588a.cc:848] argv[11]: '--2a886c8_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087296 1711130 b295d63588a.cc:848] argv[12]: '--2a886c8_hal_included_devs=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087297 1711130 b295d63588a.cc:848] argv[13]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087299 1711130 b295d63588a.cc:848] argv[14]: '--2a886c8_slice_builder_worker_addresses=10.138.0.2:8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087300 1711130 b295d63588a.cc:848] argv[15]: '--tpu_slice_builder_dump_chip=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087301 1711130 b295d63588a.cc:848] argv[16]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087302 1711130 b295d63588a.cc:848] argv[17]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087304 1711130 b295d63588a.cc:848] argv[18]: '--runtime_metric_service_port=8431'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087305 1711130 b295d63588a.cc:848] argv[19]: '--tpu_hbm_report_enable=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087306 1711130 b295d63588a.cc:848] argv[20]: '--tpu_hbm_report_frequency=5s'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087307 1711130 b295d63588a.cc:848] argv[21]: '--enable_runtime_uptime_telemetry=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087309 1711130 b295d63588a.cc:848] argv[22]: ''\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087310 1711130 b295d63588a.cc:848] argv[23]: '--xla_latency_hiding_scheduler_rerun=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087311 1711130 b295d63588a.cc:848] argv[24]: '--xla_tpu_prefer_async_allgather_to_allreduce=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087312 1711130 b295d63588a.cc:848] argv[25]: '--xla_tpu_enable_flash_attention=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087314 1711130 b295d63588a.cc:848] argv[26]: '--xla_enable_async_all_gather=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087315 1711130 b295d63588a.cc:848] argv[27]: '--xla_enable_async_collective_permute=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087616 1711130 init.cc:78] Remote crash gathering hook installed.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.087641 1711130 tpu_runtime_type_flags.cc:91] --tpu_use_tfrt not specified. Using default value: true\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.094032 1711130 tf_tpu_flags.cc:60] 2a886c8Platform is NOT registered.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.094232 1711130 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.094348 1711130 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.203517 1711130 config.cc:256] gRPC experiments enabled: max_pings_wo_data_throttle, monitoring_experiment, pick_first_new, time_caching_in_party, trace_record_callops, work_serializer_dispatch\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.212955 1711130 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 246, prefix = futex-default\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.213403 1711130 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <0A-00 00-00 00-00 00-00 80-F0 3C-43 6D-55 00-00 80-A5 59-5D 6F-7F 00-00 58-95 C1-5E 6F-7F 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = (nil), clock = 0x556d426c1420\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.249953 1711130 singleton_tpu_states_manager.cc:72] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.249962 1711130 singleton_tpu_states_manager.cc:95] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.253512 1711130 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.253519 1711130 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.253524 1711130 tpu_version_flag.cc:53] Using auto-detected TPU version TPU v5 lite\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.256625 1711130 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.256631 1711130 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.259708 1711130 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.259714 1711130 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.262801 1711130 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.262807 1711130 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.268882 1711928 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.268905 1711928 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.268920 1711928 flags_util.cc:314] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.271997 1711928 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.272002 1711928 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.272008 1711928 tpu_network_factory.cc:62] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.612421 1711930 async_driver.cc:427] [/dev/vfio/0 tpu9:pe1:1] Driver opened.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.645669 1711928 slice_builder_helper.cc:98] Current host is used as SliceBuilder master.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.645964 1711928 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.659831 1711928 master.cc:221] Successfully initialized SliceBuilder master session a17f4d88fd63acae with expected topology (1, 1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:15.660079 1711928 tpu_hal.cc:198] Starting premapped memory manager initialization...\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.101524 1711928 runtime_metric_service.cc:122] Successfully started Runtime Metric Service on port: 8431\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.101660 1711928 system.cc:1053] tpu::System initialized, current host id: 0, logical device ids: 0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.101728 1711130 tfrt_tpu_system_state.cc:213] CreateTpuSystemState: TPU initialization is successful and it took 2.83656472s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.101754 1711130 tfrt_tpu_system_state.cc:217] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.101784 1711130 tpu_host_allocator.cc:39] Premapped buffer is using alignment 512\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.101968 1711130 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.340534 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 21 instructions, modules: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.340557 1711130 autofdo_agent.cc:203] xla_tpu_autofdo_profile_dir updated to \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:26:18.340559 1711130 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/30 12:26:18\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:26:18.340559 1711130 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.340567 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.22 instructions: 21 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.341355 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.22 instructions: 142\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.341361 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.344765 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.346265 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 7.5184145ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.505575 1712133 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.512522 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 8.11555625ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.514539 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 457 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.516925 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.518627 1711929 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (52.0K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.518989 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.518992 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 52.0K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.518994 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 8.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519003 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.05M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519004 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519005 1711929 2a886c8_compiler_base.cc:3101]     program           52.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519005 1711929 2a886c8_compiler_base.cc:3101]     arguments            0B \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519006 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519007 1711929 2a886c8_compiler_base.cc:3101] Output size 20.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519007 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519014 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 212B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519015 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519016 1711929 2a886c8_compiler_base.cc:3105]     scoped               8B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519016 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 8.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519017 1711929 2a886c8_compiler_base.cc:3105]     scoped             8.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519018 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 592B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519018 1711929 2a886c8_compiler_base.cc:3105]     scoped             592B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519019 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 52.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519020 1711929 2a886c8_compiler_base.cc:3105]     overlays          52.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519020 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (0 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519034 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 6.45296275ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519112 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.22): Executable fingerprint:c0190226e36366352ca22d7e2e9151366912c050fdf35b82d07ab1b113349f2d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519114 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.22): Executable fingerprint (including data segments):89cba99eca55ab4a5f0d76ec4b1e480653cb821a975fbc8b884f46e30673494a\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519115 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.22): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:18.519345 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 180.7041905ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.287754 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 2 instructions, modules: SyncTensorsGraph.3\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.287767 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.3 instruc\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m tions: 2 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.287993 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.3 instructions: 9\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.287997 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.290166 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.290719 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 4.178138ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.291267 1712142 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.295116 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 4.0781075ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.295638 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 133 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.297888 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298603 1711929 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (31.5K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298802 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.3\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298804 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 31.5K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298807 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 8.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298814 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.03M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298815 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298816 1711929 2a886c8_compiler_base.cc:3101]     program           31.5K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298817 1711929 2a886c8_compiler_base.cc:3101]     arguments            0B \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298817 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298818 1711929 2a886c8_compiler_base.cc:3101] Output size 1.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298819 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298825 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 208B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298826 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298827 1711929 2a886c8_compiler_base.cc:3105]     scoped               4B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298827 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 8.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298828 1711929 2a886c8_compiler_base.cc:3105]     scoped             8.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298828 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 512B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298829 1711929 2a886c8_compiler_base.cc:3105]     scoped             512B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298829 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 31.5K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298830 1711929 2a886c8_compiler_base.cc:3105]     overlays          31.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298831 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (0 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298839 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 3.67527725ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298887 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.3): Executable fingerprint:ce6541084e6355686346fb3430e98c421aacd1ac6afe69b6b0af81547ecd070e\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298888 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.3): Executable fingerprint (including data segments):53292d34f1aaf3df4cdae80daa103f78473d81c0ce60a51e1354c4ab008325d1\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.298889 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.3): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:26:20.299029 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 12.529234ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:05.021528 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 530 instructions, modules: SyncTensorsGraph.542\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:05.021547 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.542 instructions: 530 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:05.026063 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.542 instructions: 1,010\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:05.026069 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:05.125673 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:05.181343 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 160.9524225ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:05.199067 1712140 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.144037 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 6.95599529375s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.446990 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 193,064 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.460135 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.538615 1711929 2a886c8_compiler_base.cc:2869] Program divided into 13 overlays without HLO functions (11.82M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679163 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.542\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679176 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 1.93G / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679182 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 106.34M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679192 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 2.53G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679193 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679194 1711929 2a886c8_compiler_base.cc:3101]     program           1.93G \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679195 1711929 2a886c8_compiler_base.cc:3101]     arguments       357.22M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679195 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679196 1711929 2a886c8_compiler_base.cc:3101] Output size 1.0K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679196 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679210 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 332B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679210 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679211 1711929 2a886c8_compiler_base.cc:3105]     scoped              76B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679212 1711929 2a886c8_compiler_base.cc:3105]     HLO temp            52B (100.0% utilization: Unpadded (52B) Padded (52B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679212 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 1.93G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679213 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          1.92G (100.0% utilization: Unpadded (1.91G) Padded (1.91G), 0.1% fragmentation (2.16M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679213 1711929 2a886c8_compiler_base.cc:3105]     overlays         11.82M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679214 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 106.34M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679215 1711929 2a886c8_compiler_base.cc:3105]     scoped           14.93M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679215 1711929 2a886c8_compiler_base.cc:3105]     HLO temp         91.41M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (91.41M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679216 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 516B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679216 1711929 2a886c8_compiler_base.cc:3105]     scoped             516B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679217 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.8K / 1.00M (104 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.679239 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 535.14977775ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.687919 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.542): Executable fingerprint:b65d67dc65a1d8ae1680747dc227516e13d6654d0536f382130be5da77ece73b\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.687923 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.542): Executable fingerprint (including data segments):89c0a98738b990d05e9113de94860d015cb552af18b28152ed3b2b59826e\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m d7ab\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.687935 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.542): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:12.816773 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 7.797135675s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:13.012796 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 3,592 instructions, modules: SyncTensorsGraph.3627\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:13.012809 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.3627 instructions: 3,592 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:13.038644 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.3627 instructions: 6,321\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:13.038653 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:13.510823 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:13.879889 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 868.4138145ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:13.956435 1716091 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:22.105529 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 8.19565680875s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.074655 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 582,480 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.108940 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.279052 1711929 2a886c8_compiler_base.cc:2869] Program divided into 33 overlays without HLO functions (35.60M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678767 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.3627\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678783 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 8.13G / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678790 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 127.08M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678799 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 8.73G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678800 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678801 1711929 2a886c8_compiler_base.cc:3101]     program           8.13G \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678802 1711929 2a886c8_compiler_base.cc:3101]     arguments       357.24M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678803 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678803 1711929 2a886c8_compiler_base.cc:3101] Output size 292.58M; shares 63.23M with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678804 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678828 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 364B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678829 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678829 1711929 2a886c8_compiler_base.cc:3105]     scoped              76B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678834 1711929 2a886c8_compiler_base.cc:3105]     HLO temp            84B (100.0% utilization: Unpadded (80B) Padded (80B), 4.8% fragmentation (4B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678835 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 8.13G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678836 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          8.10G (100.0% utilization: Unpadded (8.09G) Padded (8.09G), 0.1% fragmentation (11.11M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678836 1711929 2a886c8_compiler_base.cc:3105]     overlays         35.60M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678837 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 127.08M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678837 1711929 2a886c8_compiler_base.cc:3105]     scoped           15.56M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678838 1711929 2a886c8_compiler_base.cc:3105]     HLO temp        111.53M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (111.53M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678839 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 1.9K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678839 1711929 2a886c8_compiler_base.cc:3105]     scoped             1.9K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678840 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 4.3K / 1.00M (131 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.678863 1711929 2a886\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 1.5732019115s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.708794 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.3627): Executable fingerprint:1b2da58d1523be12e75dbb9956d6a8d8c51e4312356456a524281515e708828b\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.708802 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.3627): Executable fingerprint (including data segments):3932265337cd384f1af811717593d903f93c9837bb3207e50577c8a85d3565cd\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:23.708804 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.3627): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:24.065405 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 11.058902893s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:24.938710 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 3,097 instructions, modules: SyncTensorsGraph.3132\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:24.938729 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.3132 instructions: 3,097 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:24.963953 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.3132 instructions: 5,826\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:24.963960 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:25.511031 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:25.927399 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 990.242801ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:26.013470 1716837 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:27:26.388039 1712135 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:27:26.389651 1712135 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:27:26.388039 1712135 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:27:26.389651 1712135 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:34.188392 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 8.22976285525s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.139421 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 607,155 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.177805 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.292918 1711929 2a886c8_compiler_base.cc:2869] Program divided into 33 overlays without HLO functions (37.11M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677245 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.3132\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677258 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 8.18G / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677266 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 127.53M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677275 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 8.91G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677276 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677277 1711929 2a886c8_compiler_base.cc:3101]     program           8.18G \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677278 1711929 2a886c8_compiler_base.cc:3101]     arguments       483.59M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677278 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677279 1711929 2a886c8_compiler_base.cc:3101] Output size 292.58M; shares 189.59M with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677279 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677316 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 424B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677317 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677317 1711929 2a886c8_compiler_base.cc:3105]     scoped             100B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677318 1711929 2a886c8_compiler_base.cc:3105]     HLO temp           120B (100.0% utilization: Unpadded (120B) Padded (120B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677319 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 8.18G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677319 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          8.15G (100.0% utilization: Unpadded (8.13G) Padded (8.13G), 0.2% fragmentation (16.47M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677320 1711929 2a886c8_compiler_base.cc:3105]     overlays         37.11M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677321 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 127.53M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677321 1711929 2a886c8_compiler_base.cc:3105]     scoped           15.56M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677322 1711929 2a886c8_compiler_base.cc:3105]     HLO temp        111.97M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (111.97M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677322 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 2.4K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677323 1711929 2a886c8_compiler_base.cc:3105]     scoped             2.4K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677324 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 5.2K / 1.00M (255 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.677352 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 1.4888206685s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.711484 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.3132): Executable fingerprint:368637478226cc4fbb1a71ac64d5593e1fd0d480758c2f672deb371d204730dc\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.711493 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.3132): Executable fingerprint (including data segments):06cf8fddc960e0a9614f2c08c0beb7dab0296f1d2b89acee0561bce83926329d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:35.711495 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.3132): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:27:36.094635 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 11.16269065425s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:15.456354 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 487 instructions, modules: SyncTensorsGraph.500\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:15.456379 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.500 instructions: 487 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:15.459789 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.500 instructions: 593\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:15.459796 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:15.525166 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:15.568543 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 113.57538025ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:15.583564 1717790 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:30:20.610421 1730540 lowering_emitter.cc:4163] Successful retry compilation of add_maximum_fusion = bf16[512,64,56,56]{0,1,3,2:T(8,128)(2,1)} fusion(add_maximum_fusion.1, copy-done.9, fusion.85, get-tuple-element.45, get-tuple-element.46, get-tuple-element.43, get-tuple-element.44), kind=kOutput, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_2.43 = f32[512,64,56,56]{0,1,3,2:T(8,128)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_5.22 = f32[64]{0:T(128)S(1)} parameter(5)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_6.15 = f32[64]{0:T(128)S(1)} parameter(6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.9 = f32[512,64,56,56]{0,1,3,2:T(8,128)} fusion(param_2.43, param_5.22, param_6.15), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_0.64 = f32[512,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_2.44 = f32[64]{0:T(128)S(1)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.104 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_2.44), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     multiply.68 = f32[512,64,56,56]{0,1,3,2:T(8,128)} multiply(param_0.64, broadcast.104)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_1.64 = f32[64]{0:T(128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.100 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_1.64), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     add.49 = f32[512,64,56,56]{0,1,3,2:T(8,128)} add(multiply.68, broadcast.100)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     constant.153 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.114 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.153), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT maximum.4 = f32[512,64,56,56]{0,1,3,2:T(8,128)} maximum(add.49, broadcast.114)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_1.63 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.101 = f32[64,64,3,3]{1,0,3,2:T(8,128)} fusion(param_1.63), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     bitcast_input.10 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT bitcast.14 = f32[64,64,3,3]{1,0,3,2:T(8,128)} bitcast(bitcast_input.10)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   convolution.2 = f32[512,64,56,56]{0,1,3,2:T(8,128)} convolution(fusion.9, fusion.101), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_4.24 = f32[64]{0:T(128)S(1)} parameter(4)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.96 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_4.24), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   multiply.64 = f32[512,64,56,56]{0,1,3,2:T(8,128)} multiply(convolution.2, broadcast.96)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_3.37 = f32[64]{0:T(128)S(1)} parameter(3)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.95 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_3.37), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.45 = f32[512,64,56,56]{0,1,3,2:T(8,128)} add(multiply.64, broadcast.95)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_0.61 = f32[512,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.44 = f32[512,64,56,56]{0,1,3,2:T(8,128)} add(add.45, param_0.61)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   constant.39.clone.7 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.150.clone.6 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.39.clone.7), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   ROOT maximum.2 = bf16[512,64,56,56]{0,1,3,2:T(8,128)(2,1)} maximum(add.44, broadcast.150.clone.6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m } after 2 retries\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:30:20.610421 1730540 lowering_emitter.cc:4163] Successful retry compilation of add_maximum_fusion = bf16[512,64,56,56]{0,1,3,2:T(8,128)(2,1)} fusion(add_maximum_fusion.1, copy-done.9, fusion.85, get-tuple-element.45, get-tuple-element.46, get-tuple-element.43, get-tuple-element.44), kind=kOutput, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_2.43 = f32[512,64,56,56]{0,1,3,2:T(8,128)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_5.22 = f32[64]{0:T(128)S(1)} parameter(5)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_6.15 = f32[64]{0:T(128)S(1)} parameter(6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.9 = f32[512,64,56,56]{0,1,3,2:T(8,128)} fusion(param_2.43, param_5.22, param_6.15), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_0.64 = f32[512,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_2.44 = f32[64]{0:T(128)S(1)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.104 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_2.44), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     multiply.68 = f32[512,64,56,56]{0,1,3,2:T(8,128)} multiply(param_0.64, broadcast.104)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_1.64 = f32[64]{0:T(128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.100 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_1.64), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     add.49 = f32[512,64,56,56]{0,1,3,2:T(8,128)} add(multiply.68, broadcast.100)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     constant.153 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.114 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.153), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT maximum.4 = f32[512,64,56,56]{0,1,3,2:T(8,128)} maximum(add.49, broadcast.114)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_1.63 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.101 = f32[64,64,3,3]{1,0,3,2:T(8,128)} fusion(param_1.63), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     bitcast_input.10 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT bitcast.14 = f32[64,64,3,3]{1,0,3,2:T(8,128)} bitcast(bitcast_input.10)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   convolution.2 = f32[512,64,56,56]{0,1,3,2:T(8,128)} convolution(fusion.9, fusion.101), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_4.24 = f32[64]{0:T(128)S(1)} parameter(4)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.96 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_4.24), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   multiply.64 = f32[512,64,56,56]{0,1,3,2:T(8,128)} multiply(convolution.2, broadcast.96)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_3.37 = f32[64]{0:T(128)S(1)} parameter(3)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.95 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_3.37), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.45 = f32[512,64,56,56]{0,1,3,2:T(8,128)} add(multiply.64, broadcast.95)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_0.61 = f32[512,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.44 = f32[512,64,56,56]{0,1,3,2:T(8,128)} add(add.45, param_0.61)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   constant.39.clone.7 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.150.clone.6 = f32[512,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.39.clone.7), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   ROOT maximum.2 = bf16[512,64,56,56]{0,1,3,2:T(8,128)(2,1)} maximum(add.44, broadcast.150.clone.6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m } after 2 retries\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:20.640020 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 5.06646935825s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:20.907925 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 184,026 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:20.920384 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:20.982559 1711929 2a886c8_compiler_base.cc:2869] Program divided into 12 overlays without HLO functions (11.26M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125417 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.500\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125433 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 1.93G / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125440 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 102.45M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125456 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 2.53G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125458 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125458 1711929 2a886c8_compiler_base.cc:3101]     program           1.93G \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125459 1711929 2a886c8_compiler_base.cc:3101]     arguments       357.22M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125460 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125461 1711929 2a886c8_compiler_base.cc:3101] Output size 19.98M; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125461 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125476 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 384B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125476 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125477 1711929 2a886c8_compiler_base.cc:3105]     scoped             116B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125478 1711929 2a886c8_compiler_base.cc:3105]     HLO temp            64B (100.0% utilization: Unpadded (64B) Padded (64B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125479 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 1.93G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125480 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          1.92G (100.0% utilization: Unpadded (1.91G) Padded (1.91G), 0.1% fragmentation (2.27M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125481 1711929 2a886c8_compiler_base.cc:3105]     overlays         11.26M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125482 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 102.45M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125482 1711929 2a886c8_compiler_base.cc:3105]     scoped           15.04M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125483 1711929 2a886c8_compiler_base.cc:3105]     HLO temp         87.41M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (87.41M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125484 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 552B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125485 1711929 2a886c8_compiler_base.cc:3105]     scoped             552B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125486 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.8K / 1.00M (104 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.125535 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 485.43386175ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.134207 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.500): Executable fingerprint:55bb528e46686c90d97faf59195edfa6013ccbbc3b487c3de36627f3f5871442\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.134212 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.500): Executable fingerprint (including data segments):dd7d26087cb8cb456eb053e5042d6f487af237ef00faa6af136943c657091b76\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.134213 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.500): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.276600 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 5.822686275s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.412576 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 11 instructions, modules: SyncTensorsGraph.13\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.412600 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.13 instructions: 11 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.413040 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.13 instructions: 58\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.413044 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.417839 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.419745 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 8.3378465ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.421112 1730553 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.432018 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 11.57585175ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.433232 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 341 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.435781 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.436996 1711929 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (44.5K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437465 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.13\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437467 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 92.5K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437470 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 14.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437478 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.09M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437479 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437480 1711929 2a886c8_compiler_base.cc:3101]     program           92.5K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437480 1711929 2a886c8_compiler_base.cc:3101]     arguments          4.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437481 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437482 1711929 2a886c8_compiler_base.cc:3101] Output size 1.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437482 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437491 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 220B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437492 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437493 1711929 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437494 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 92.5K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437494 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          48.0K (33.9% utilization: Unpadded (520B) Padded (1.5K), 96.9% fragmentation (46.5K))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437495 1711929 2a886c8_compiler_base.cc:3105]     overlays          44.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437496 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 14.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437496 1711929 2a886c8_compiler_base.cc:3105]     scoped            14.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437497 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 516B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437497 1711929 2a886c8_compiler_base.cc:3105]     scoped             516B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437498 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (1 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437507 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 5.38858025ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437573 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.13): Executable fingerprint:31bf645b9edde791e5077ce0c485321577c04788c42485ce2d39006db57f9ae8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437575 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.13): Executable fingerprint (including data segments):bd0ebeb0da9102be15a563d1ce59d670cbec2ce77767d55f944e07ac1cccafbd\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437576 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.13): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.437959 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 26.622601ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.442526 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 10 instructions, modules: SyncTensorsGraph.12\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.442532 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.12 instructions: 10 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.442890 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.12 instruct\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m ions: 57\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.442894 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.446531 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.447853 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 6.65483325ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.448796 1730553 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.459889 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 11.4921325ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.461193 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 341 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.463584 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.464745 1711929 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (44.5K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465226 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.12\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465228 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 92.5K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465231 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 14.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465238 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.09M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465239 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465240 1711929 2a886c8_compiler_base.cc:3101]     program           92.5K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465241 1711929 2a886c8_compiler_base.cc:3101]     arguments          4.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465241 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465242 1711929 2a886c8_compiler_base.cc:3101] Output size 1.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465243 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465251 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 220B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465251 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465252 1711929 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465253 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 92.5K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465253 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          48.0K (33.9% utilization: Unpadded (520B) Padded (1.5K), 96.9% fragmentation (46.5K))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465254 1711929 2a886c8_compiler_base.cc:3105]     overlays          44.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465254 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 14.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465255 1711929 2a886c8_compiler_base.cc:3105]     scoped            14.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465256 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 516B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465256 1711929 2a886c8_compiler_base.cc:3105]     scoped             516B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465257 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (1 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465265 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 5.28717025ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465324 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.12): Executable fingerprint:a5752935aa6d3a21fa4a102aa778b14169b85ed7373ee144975745376e842a69\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465326 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.12): Executable fingerprint (including data segments):d4f650d0d2129f5760b021e7f8c527920960a7d97a93cbf83a8b6f1a0a6f9957\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465327 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.12): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:21.465691 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 24.5449675ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:36.859883 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 487 instructions, modules: SyncTensorsGraph.500\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:36.859927 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.500 instructions: 487 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:36.863191 1711130 2a886c8_compil\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m er_base.cc:6229] HLO optimizing module: SyncTensorsGraph.500 instructions: 593\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:36.863210 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:36.925076 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:36.964359 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 105.828465ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:36.976734 1730553 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:30:41.985207 1730561 lowering_emitter.cc:4163] Successful retry compilation of add_maximum_fusion = bf16[280,64,56,56]{0,1,3,2:T(8,128)(2,1)} fusion(add_maximum_fusion.1, copy-done.5, fusion.81, get-tuple-element.56, get-tuple-element.57, get-tuple-element.54, get-tuple-element.55), kind=kOutput, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_2.29 = f32[280,64,56,56]{0,1,3,2:T(8,128)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_5.15 = f32[64]{0:T(128)S(1)} parameter(5)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_6.11 = f32[64]{0:T(128)S(1)} parameter(6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.7 = f32[280,64,56,56]{0,1,3,2:T(8,128)} fusion(param_2.29, param_5.15, param_6.11), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_0.43 = f32[280,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_2.30 = f32[64]{0:T(128)S(1)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.104 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_2.30), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     multiply.68 = f32[280,64,56,56]{0,1,3,2:T(8,128)} multiply(param_0.43, broadcast.104)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_1.39 = f32[64]{0:T(128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.100 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_1.39), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     add.49 = f32[280,64,56,56]{0,1,3,2:T(8,128)} add(multiply.68, broadcast.100)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     constant.159 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.114 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.159), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT maximum.4 = f32[280,64,56,56]{0,1,3,2:T(8,128)} maximum(add.49, broadcast.114)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_1.38 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.100 = f32[64,64,3,3]{1,0,3,2:T(8,128)} fusion(param_1.38), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     bitcast_input.12 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT bitcast.14 = f32[64,64,3,3]{1,0,3,2:T(8,128)} bitcast(bitcast_input.12)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   convolution.1 = f32[280,64,56,56]{0,1,3,2:T(8,128)} convolution(fusion.7, fusion.100), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_4.17 = f32[64]{0:T(128)S(1)} parameter(4)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.96 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_4.17), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   multiply.64 = f32[280,64,56,56]{0,1,3,2:T(8,128)} multiply(convolution.1, broadcast.96)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_3.26 = f32[64]{0:T(128)S(1)} parameter(3)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.95 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_3.26), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.45 = f32[280,64,56,56]{0,1,3,2:T(8,128)} add(multiply.64, broadcast.95)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_0.40 = f32[280,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.44 = f32[280,64,56,56]{0,1,3,2:T(8,128)} add(add.45, param_0.40)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   constant.39.clone.9 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.150.clone.6 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.39.clone.9), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   ROOT maximum.2 = bf16[280,64,56,56]{0,1,3,2:T(8,128)(2,1)} maximum(add.44, broadcast.150.clone.6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m } after 2 retries\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:30:41.985207 1730561 lowering_emitter.cc:4163] Successful retry compilation of add_maximum_fusion = bf16[280,64,56,56]{0,1,3,2:T(8,128)(2,1)} fusion(add_maximum_fusion.1, copy-done.5, fusion.81, get-tuple-element.56, get-tuple-element.57, get-tuple-element.54, get-tuple-element.55), kind=kOutput, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_2.29 = f32[280,64,56,56]{0,1,3,2:T(8,128)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_5.15 = f32[64]{0:T(128)S(1)} parameter(5)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_6.11 = f32[64]{0:T(128)S(1)} parameter(6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.7 = f32[280,64,56,56]{0,1,3,2:T(8,128)} fusion(param_2.29, param_5.15, param_6.11), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_0.43 = f32[280,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_2.30 = f32[64]{0:T(128)S(1)} parameter(2)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.104 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_2.30), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     multiply.68 = f32[280,64,56,56]{0,1,3,2:T(8,128)} multiply(param_0.43, broadcast.104)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     param_1.39 = f32[64]{0:T(128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.100 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_1.39), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     add.49 = f32[280,64,56,56]{0,1,3,2:T(8,128)} add(multiply.68, broadcast.100)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     constant.159 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     broadcast.114 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.159), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT maximum.4 = f32[280,64,56,56]{0,1,3,2:T(8,128)} maximum(add.49, broadcast.114)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_1.38 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   fusion.100 = f32[64,64,3,3]{1,0,3,2:T(8,128)} fusion(param_1.38), kind=kLoop, calls=\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   {\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     bitcast_input.12 = f32[64,64,3,3]{1,0,3,2:T(8,128)S(1)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m     ROOT bitcast.14 = f32[64,64,3,3]{1,0,3,2:T(8,128)} bitcast(bitcast_input.12)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   }\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   convolution.1 = f32[280,64,56,56]{0,1,3,2:T(8,128)} convolution(fusion.7, fusion.100), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_4.17 = f32[64]{0:T(128)S(1)} parameter(4)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.96 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_4.17), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   multiply.64 = f32[280,64,56,56]{0,1,3,2:T(8,128)} multiply(convolution.1, broadcast.96)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_3.26 = f32[64]{0:T(128)S(1)} parameter(3)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.95 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(param_3.26), dimensions={1}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.45 = f32[280,64,56,56]{0,1,3,2:T(8,128)} add(multiply.64, broadcast.95)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   param_0.40 = f32[280,64,56,56]{0,1,3,2:T(8,128)} parameter(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   add.44 = f32[280,64,56,56]{0,1,3,2:T(8,128)} add(add.45, param_0.40)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   constant.39.clone.9 = f32[]{:T(128)} constant(0)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   broadcast.150.clone.6 = f32[280,64,56,56]{0,1,3,2:T(8,128)} broadcast(constant.39.clone.9), dimensions={}\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m   ROOT maximum.2 = bf16[280,64,56,56]{0,1,3,2:T(8,128)(2,1)} maximum(add.44, broadcast.150.clone.6)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m } after 2 retries\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.114163 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 5.14635790325s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.423371 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 203,728 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.436016 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.499074 1711929 2a886c8_compiler_base.cc:2869] Program divided into 14 overlays without HLO functions (12.47M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657558 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.500\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657580 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 1.45G / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657587 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 124.82M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657605 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 1.94G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657606 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657607 1711929 2a886c8_compiler_base.cc:3101]     program           1.45G \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657607 1711929 2a886c8_compiler_base.cc:3101]     arguments       246.97M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657608 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657609 1711929 2a886c8_compiler_base.cc:3101] Output size 11.00M; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657610 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657623 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 292B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657623 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657624 1711929 2a886c8_compiler_base.cc:3105]     scoped              28B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657624 1711929 2a886c8_compiler_base.cc:3105]     HLO temp            60B (100.0% utilization: Unpadded (60B) Padded (60B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657626 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 1.45G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657626 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          1.44G (72.9% utilization: Unpadded (1.05G) Padded (1.44G), 0.2% fragmentation (2.24M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657627 1711929 2a886c8_compiler_base.cc:3105]     overlays         12.47M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657628 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 124.82M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657628 1711929 2a886c8_compiler_base.cc:3105]     scoped           15.04M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657629 1711929 2a886c8_compiler_base.cc:3105]     HLO temp        109.78M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (109.78M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657629 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 540B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657630 1711929 2a886c8_compiler_base.cc:3105]     scoped             540B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657631 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.8K / 1.00M (104 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.657665 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 543.4292445ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.667048 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.500): Executable fingerprint:517d018cf427afeb17b5cc00dde6f874b2a8fe1e575a2aebe5c3b0d158e182ad\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.667052 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.500): Executable fingerprint (including data segments):dd00139eed873e03a5e3eaf69bdc27aa7df87b8f0684cafcc275b5e0c1e9c22c\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.667053 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.500): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:42.822376 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 5.96480442075s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.064840 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 11 instructions, modules: SyncTensorsGraph.13\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.064877 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.13 instructions: 11 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.065300 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.13 instructions: 58\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.065304 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.069344 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.071007 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 7.35923475ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.072219 1730569 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.083443 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 11.7557525ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.084693 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 351 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.087156 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088224 1711929 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (45.0K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088688 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.13\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088690 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 93.0K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088693 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 13.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088701 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.09M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088702 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088703 1711929 2a886c8_compiler_base.cc:3101]     program           93.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088704 1711929 2a886c8_compiler_base.cc:3101]     arguments          3.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088705 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088705 1711929 2a886c8_compiler_base.cc:3101] Output size 1.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088706 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088715 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 220B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088716 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088717 1711929 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088718 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 93.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088719 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          48.0K (33.9% utilization: Unpadded (520B) Padded (1.5K), 96.9% fragmentation (46.5K))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088720 1711929 2a886c8_compiler_base.cc:3105]     overlays          45.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088720 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 13.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088721 1711929 2a886c8_compiler_base.cc:3105]     scoped            13.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088722 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 516B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088723 1711929 2a886c8_compiler_base.cc:3105]     scoped             516B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088724 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (1 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088732 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 5.2277ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088796 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.13): Executable fingerprint:5a65b48bf13cda030361667773e704667d87a4b4b59bb01778b570a85e9c5d02\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088799 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.13): Executable fingerprint (including data segments):ae69357acba9558d9016a7ce6bbd1c901a6152c2c03df1831d9e1a8a72c89f33\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.088800 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.13): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.089159 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 25.5794195ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.093524 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 10 instructions, modules: SyncTensorsGraph.12\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.093530 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.12 instructions: 10 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.093878 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.12 instructio\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m ns: 57\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.093882 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.097110 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.098265 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 6.01423125ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.099121 1730569 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.109236 1711929 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 10.39803ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.110464 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 351 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.112742 1711929 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.113869 1711929 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (45.0K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114327 1711929 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.12\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114329 1711929 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 93.0K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114331 1711929 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 13.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114338 1711929 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.09M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114339 1711929 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114340 1711929 2a886c8_compiler_base.cc:3101]     program           93.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114341 1711929 2a886c8_compiler_base.cc:3101]     arguments          3.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114341 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114342 1711929 2a886c8_compiler_base.cc:3101] Output size 1.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114343 1711929 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114351 1711929 2a886c8_compiler_base.cc:3105] Program sflag requirement 220B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114351 1711929 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114352 1711929 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114353 1711929 2a886c8_compiler_base.cc:3105] Program hbm requirement 93.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114353 1711929 2a886c8_compiler_base.cc:3105]     HLO temp          48.0K (33.9% utilization: Unpadded (520B) Padded (1.5K), 96.9% fragmentation (46.5K))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114354 1711929 2a886c8_compiler_base.cc:3105]     overlays          45.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114355 1711929 2a886c8_compiler_base.cc:3105] Program vmem requirement 13.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114355 1711929 2a886c8_compiler_base.cc:3105]     scoped            13.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114356 1711929 2a886c8_compiler_base.cc:3105] Program smem requirement 516B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114356 1711929 2a886c8_compiler_base.cc:3105]     scoped             516B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114357 1711929 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (1 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114382 1711929 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 5.12105975ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114440 1711929 isa_program.cc:328] (HLO module SyncTensorsGraph.12): Executable fingerprint:0552236a02b9a09a9ba1d584e69051b95fa1b20d8a0060328cfe939bfbc625ee\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114442 1711929 isa_program.cc:332] (HLO module SyncTensorsGraph.12): Executable fingerprint (including data segments):92f1f02d7681e5d3949b1da5317c76335a2043f4e5190d38132486a7e83186d6\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114443 1711929 isa_program.cc:335] (HLO module SyncTensorsGraph.12): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:30:43.114792 1711130 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 22.58648375ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 13:38:44.024366 1711130 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 3,096 instructions, modules: SyncTensorsGraph.3131\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 13:38:44.024419 1711130 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.3131 instructions: 3,096 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 13:38:44.051623 1711130 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.3131 instructions: 5,825\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 13:38:44.051644 1711130 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 13:38:44.603404 1711130 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 13:38:45.016221 1711130 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 993.3646265ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 13:38:45.093350 1730569 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 13:38:45.439791 2044903 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 13:38:45.440848 2044903 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 13:38:45.439791 2044903 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 13:38:45.440848 2044903 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1711130, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Using cached datasets.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2748559)\u001b[0m Test set size: 43288\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1e4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision import datasets, models, tv_tensors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962db9a",
   "metadata": {},
   "source": [
    "## Look at our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584152da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b61853d3d94d55b33b52ac89a15b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f785005f2d241bfa52145995afd74fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2637439f07f74bb0abdb4493e333e020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d1c8069f3148ebbda109b858cc9652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986da1cdcbe74ac88a0ad567f999d139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763ba49419e54d7f830f94486a1bd540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293108a63ec248059c2c87b8ccdfb6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_raw_dataset(split):\n",
    "  dataset = load_dataset(\"flwrlabs/celeba\", split=split, trust_remote_code=True, cache_dir='cache')\n",
    "  return dataset.with_format(\"torch\")\n",
    "\n",
    "train = load_raw_dataset(\"train\")\n",
    "test = load_raw_dataset(\"test\")\n",
    "valid = load_raw_dataset(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12ff363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 8192\n",
      "Test set: 1000\n",
      "Valid set: 985\n",
      "Total unique IDs: 10177\n"
     ]
    }
   ],
   "source": [
    "train_set = train.unique('celeb_id') # type: ignore\n",
    "test_set = test.unique('celeb_id') # type: ignore\n",
    "valid_set = valid.unique('celeb_id') # type: ignore\n",
    "print(f\"Train set: {len(train_set)}\")\n",
    "print(f\"Test set: {len(test_set)}\")\n",
    "print(f\"Valid set: {len(valid_set)}\")\n",
    "print(f\"Total unique IDs: {len(set(train_set) | set(test_set) | set(valid_set))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6aee5",
   "metadata": {},
   "source": [
    "Problem: there is no label overlap between train/test/valid sets.\n",
    "\n",
    "We can't use the validation set to validate accuracy because it contains\n",
    "completely unseen labels.\n",
    "\n",
    "For the purpose of this specific convergence test, we'll do our own split\n",
    "such that the same labels show up in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d59388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached datasets.\n",
      "Number of classes: 10177\n",
      "Train set size: 159267\n",
      "Test set size: 43288\n"
     ]
    }
   ],
   "source": [
    "from data_util import download_dataset_custom_split_cached\n",
    "\n",
    "train_dataset, test_dataset, NUM_CLASSES = download_dataset_custom_split_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28dd816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2489/2489 [00:42<00:00, 58.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in train loader: 10133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 677/677 [00:11<00:00, 56.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in test loader: 10133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8, shuffle=False)  # type: ignore\n",
    "\n",
    "# Enumerate train loader to find number of unique labels\n",
    "train_unique_labels = set()\n",
    "for batch in tqdm(train_loader):\n",
    "    labels = batch['label']\n",
    "    train_unique_labels.update(set(labels.flatten().numpy().tolist()))\n",
    "print(f\"Number of unique labels in train loader: {len(train_unique_labels)}\")\n",
    "# Enumerate test loader to find number of unique labels\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=8, shuffle=False)  # type: ignore\n",
    "test_unique_labels = set()\n",
    "for batch in tqdm(test_loader):\n",
    "    labels = batch['label']\n",
    "    test_unique_labels.update(set(labels.flatten().numpy().tolist()))\n",
    "print(f\"Number of unique labels in test loader: {len(test_unique_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf0c841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10177\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = max(train_unique_labels) + 1\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "assert train_unique_labels == test_unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378f904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176],\n",
       " [10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(train_unique_labels))[-10:], sorted(list(test_unique_labels))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b473163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Actor (PID: 2748362) using cuda (NVIDIA A100-SXM4-80GB)\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "\n",
      "/home/yifeit_google_com/resnet18\n"
     ]
    }
   ],
   "source": [
    "from actors import get_gpu_actor\n",
    "\n",
    "gpu_actor = get_gpu_actor()\n",
    "print(ray.get(gpu_actor.print_model_architecture.remote()))\n",
    "print(ray.get(gpu_actor.get_cwd.remote()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba9c39",
   "metadata": {},
   "source": [
    "## Convergence test\n",
    "\n",
    "Now let's run a proper training run with:\n",
    "\n",
    "- Learning rate: `1e-3` (unchanged)\n",
    "- Batch size: 512 (unchanged)\n",
    "- Training set: training subset of the remixed `flwrlabs/celeba`\n",
    "- Validation set (used to compute validation accuracy): testing subset of the remixed `flwrlabs/celeba`\n",
    "- Measure:\n",
    "  - val-loss\n",
    "  - val-top1-acc\n",
    "\n",
    "If TPU is materially worse on val-top1-acc with same LR/batchsize, that's a\n",
    "material datapoint and we probably want to bring it back to the TPU team to see\n",
    "if that's also expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Actor (PID: 2748559) using cuda (NVIDIA A100-SXM4-80GB)\n",
      "TPU Actor (PID: 1711130) using xla:0 (v5litepod-8)\n",
      " GPU and TPU models have identical state dicts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312it [04:22,  1.19it/s, Epoch 0 | GPU Loss: 9.198456 | TPU Loss: 8.956905 | GPU Validation: {'avg_valid_loss': 9.155792225108428, 'top1_accuracy': 0.0001386065422287932, 'top5_accuracy': 0.0008316392533727592} | TPU Validation: {'avg_valid_loss': 9.017434793360094, 'top1_accuracy': 0.00023101090371465533, 'top5_accuracy': 0.0011550545185732767}]\n",
      "312it [03:36,  1.44it/s, Epoch 1 | GPU Loss: 8.483397 | TPU Loss: 8.139778 | GPU Validation: {'avg_valid_loss': 8.706157403833727, 'top1_accuracy': 0.0005544261689151728, 'top5_accuracy': 0.0025411199408612085} | TPU Validation: {'avg_valid_loss': 8.171939131792854, 'top1_accuracy': 0.00129366106080207, 'top5_accuracy': 0.005613564960266124}]\n",
      "312it [03:37,  1.43it/s, Epoch 2 | GPU Loss: 7.278735 | TPU Loss: 7.667201 | GPU Validation: {'avg_valid_loss': 7.365274261025822, 'top1_accuracy': 0.006630012936610608, 'top5_accuracy': 0.026081131029384585} | TPU Validation: {'avg_valid_loss': 7.786041876849006, 'top1_accuracy': 0.0024487155793753465, 'top5_accuracy': 0.010742007022731473}]\n",
      "312it [03:37,  1.43it/s, Epoch 3 | GPU Loss: 5.773001 | TPU Loss: 7.094642 | GPU Validation: {'avg_valid_loss': 6.22944818945492, 'top1_accuracy': 0.03837091110700425, 'top5_accuracy': 0.11786176307521715} | TPU Validation: {'avg_valid_loss': 7.410814891142004, 'top1_accuracy': 0.0048512289780077615, 'top5_accuracy': 0.019982443171317685}]\n",
      "312it [03:39,  1.42it/s, Epoch 4 | GPU Loss: 4.262207 | TPU Loss: 6.362946 | GPU Validation: {'avg_valid_loss': 5.373288395825554, 'top1_accuracy': 0.0980641286268712, 'top5_accuracy': 0.24487155793753465} | TPU Validation: {'avg_valid_loss': 6.704625718733844, 'top1_accuracy': 0.01660968397708372, 'top5_accuracy': 0.059392903345037884}]\n",
      "312it [03:38,  1.43it/s, Epoch 5 | GPU Loss: 3.143527 | TPU Loss: 5.411281 | GPU Validation: {'avg_valid_loss': 4.500951026467716, 'top1_accuracy': 0.1954583256329699, 'top5_accuracy': 0.39634540750323416} | TPU Validation: {'avg_valid_loss': 5.90827725354363, 'top1_accuracy': 0.05271668822768435, 'top5_accuracy': 0.1525596008131584}]\n",
      "312it [03:37,  1.44it/s, Epoch 6 | GPU Loss: 2.329421 | TPU Loss: 4.471449 | GPU Validation: {'avg_valid_loss': 3.954011485155891, 'top1_accuracy': 0.28083995564590647, 'top5_accuracy': 0.5034189613749769} | TPU Validation: {'avg_valid_loss': 5.310048333336325, 'top1_accuracy': 0.10226852707447792, 'top5_accuracy': 0.24882184439105526}]\n",
      "312it [03:37,  1.44it/s, Epoch 7 | GPU Loss: 1.650749 | TPU Loss: 3.702465 | GPU Validation: {'avg_valid_loss': 3.7030959045185763, 'top1_accuracy': 0.34469136943263723, 'top5_accuracy': 0.5691184623914248} | TPU Validation: {'avg_valid_loss': 4.998960410847383, 'top1_accuracy': 0.14449732027351692, 'top5_accuracy': 0.3194418776566254}]\n",
      "312it [03:41,  1.41it/s, Epoch 8 | GPU Loss: 1.168891 | TPU Loss: 3.005742 | GPU Validation: {'avg_valid_loss': 3.908711150113274, 'top1_accuracy': 0.34372112363703566, 'top5_accuracy': 0.5686564405839956} | TPU Validation: {'avg_valid_loss': 4.795178200216855, 'top1_accuracy': 0.17376640177416375, 'top5_accuracy': 0.3658750693032711}]\n",
      "312it [03:36,  1.44it/s, Epoch 9 | GPU Loss: 0.887051 | TPU Loss: 2.593833 | GPU Validation: {'avg_valid_loss': 3.702589719435748, 'top1_accuracy': 0.3924413232304565, 'top5_accuracy': 0.6151358344113842} | TPU Validation: {'avg_valid_loss': 4.745279155058019, 'top1_accuracy': 0.19869247828497505, 'top5_accuracy': 0.39897893180558125}]\n",
      "312it [03:38,  1.43it/s, Epoch 10 | GPU Loss: 0.516824 | TPU Loss: 2.127256 | GPU Validation: {'avg_valid_loss': 3.6539182887357824, 'top1_accuracy': 0.41605063759009425, 'top5_accuracy': 0.6368970615413048} | TPU Validation: {'avg_valid_loss': 4.75719804763794, 'top1_accuracy': 0.2237802624283866, 'top5_accuracy': 0.4322445019404916}]\n",
      "312it [03:36,  1.44it/s, Epoch 11 | GPU Loss: 0.399111 | TPU Loss: 1.714759 | GPU Validation: {'avg_valid_loss': 3.745713789322797, 'top1_accuracy': 0.4331916466457217, 'top5_accuracy': 0.6476621696544077} | TPU Validation: {'avg_valid_loss': 5.234249019622803, 'top1_accuracy': 0.2139391979301423, 'top5_accuracy': 0.4148262798004066}]\n",
      "312it [03:36,  1.44it/s, Epoch 12 | GPU Loss: 0.242781 | TPU Loss: 1.369766 | GPU Validation: {'avg_valid_loss': 3.8861326133503633, 'top1_accuracy': 0.44120772500462024, 'top5_accuracy': 0.6497643688782111} | TPU Validation: {'avg_valid_loss': 5.496821324965533, 'top1_accuracy': 0.21363888375531326, 'top5_accuracy': 0.4171825910182961}]\n",
      "312it [03:36,  1.44it/s, Epoch 13 | GPU Loss: 0.160654 | TPU Loss: 1.229260 | GPU Validation: {'avg_valid_loss': 3.7289243950563318, 'top1_accuracy': 0.47327203844021437, 'top5_accuracy': 0.6758223988172242} | TPU Validation: {'avg_valid_loss': 5.234553976619945, 'top1_accuracy': 0.2504620218074293, 'top5_accuracy': 0.45832563296987616}]\n",
      "312it [03:38,  1.43it/s, Epoch 14 | GPU Loss: 0.118519 | TPU Loss: 0.965665 | GPU Validation: {'avg_valid_loss': 3.663064810808967, 'top1_accuracy': 0.4950563666605064, 'top5_accuracy': 0.6908381075586768} | TPU Validation: {'avg_valid_loss': 5.622563233095057, 'top1_accuracy': 0.2502772130844576, 'top5_accuracy': 0.45793291443356127}]\n",
      "312it [03:36,  1.44it/s, Epoch 15 | GPU Loss: 0.076846 | TPU Loss: 0.662228 | GPU Validation: {'avg_valid_loss': 3.691848679149852, 'top1_accuracy': 0.5045047126224358, 'top5_accuracy': 0.697745333579745} | TPU Validation: {'avg_valid_loss': 5.767018177930047, 'top1_accuracy': 0.2675337275919423, 'top5_accuracy': 0.4766216965440769}]\n",
      "312it [03:37,  1.44it/s, Epoch 16 | GPU Loss: 0.074276 | TPU Loss: 0.619639 | GPU Validation: {'avg_valid_loss': 3.770793780158548, 'top1_accuracy': 0.5086629088892995, 'top5_accuracy': 0.700425060062835} | TPU Validation: {'avg_valid_loss': 6.155411293927361, 'top1_accuracy': 0.27148401404546296, 'top5_accuracy': 0.4764137867307337}]\n",
      "312it [03:38,  1.43it/s, Epoch 17 | GPU Loss: 0.097390 | TPU Loss: 0.438510 | GPU Validation: {'avg_valid_loss': 3.98421394404243, 'top1_accuracy': 0.4984060247643689, 'top5_accuracy': 0.6918083533542784} | TPU Validation: {'avg_valid_loss': 6.160023902444278, 'top1_accuracy': 0.28199501016447975, 'top5_accuracy': 0.49332378488264644}]\n",
      "312it [03:36,  1.44it/s, Epoch 18 | GPU Loss: 0.148049 | TPU Loss: 0.315000 | GPU Validation: {'avg_valid_loss': 4.207637820524328, 'top1_accuracy': 0.47907041212345225, 'top5_accuracy': 0.6787331362040289} | TPU Validation: {'avg_valid_loss': 6.226018249287325, 'top1_accuracy': 0.2953705414895583, 'top5_accuracy': 0.5064221031232674}]\n",
      "312it [03:41,  1.41it/s, Epoch 19 | GPU Loss: 0.214416 | TPU Loss: 0.349607 | GPU Validation: {'avg_valid_loss': 4.337636947631836, 'top1_accuracy': 0.467496765847348, 'top5_accuracy': 0.6726575494363334} | TPU Validation: {'avg_valid_loss': 6.402967262268066, 'top1_accuracy': 0.29386897061541306, 'top5_accuracy': 0.4969968582517095}]\n",
      "312it [03:37,  1.43it/s, Epoch 20 | GPU Loss: 0.175558 | TPU Loss: 0.285103 | GPU Validation: {'avg_valid_loss': 4.231528248506434, 'top1_accuracy': 0.49159120310478654, 'top5_accuracy': 0.6876963592681574} | TPU Validation: {'avg_valid_loss': 6.728362891253303, 'top1_accuracy': 0.2769358713731288, 'top5_accuracy': 0.4840833487340602}]\n",
      "312it [03:38,  1.43it/s, Epoch 21 | GPU Loss: 0.111565 | TPU Loss: 0.225337 | GPU Validation: {'avg_valid_loss': 4.112476048750036, 'top1_accuracy': 0.5116198484568472, 'top5_accuracy': 0.7028275734614674} | TPU Validation: {'avg_valid_loss': 6.654755412831026, 'top1_accuracy': 0.2966411014599889, 'top5_accuracy': 0.5046433191646645}]\n",
      "312it [03:36,  1.44it/s, Epoch 22 | GPU Loss: 0.073308 | TPU Loss: 0.206068 | GPU Validation: {'avg_valid_loss': 4.062531518936157, 'top1_accuracy': 0.5207678802439475, 'top5_accuracy': 0.7087183515061911} | TPU Validation: {'avg_valid_loss': 6.542038794124828, 'top1_accuracy': 0.3175013860654223, 'top5_accuracy': 0.5241868416189244}]\n",
      "312it [03:37,  1.43it/s, Epoch 23 | GPU Loss: 0.078079 | TPU Loss: 0.222839 | GPU Validation: {'avg_valid_loss': 4.058616085613475, 'top1_accuracy': 0.5223387543892072, 'top5_accuracy': 0.7113980779892811} | TPU Validation: {'avg_valid_loss': 6.683139099794276, 'top1_accuracy': 0.313320088708187, 'top5_accuracy': 0.5215764184069488}]\n",
      "312it [03:37,  1.43it/s, Epoch 24 | GPU Loss: 0.067414 | TPU Loss: 0.218195 | GPU Validation: {'avg_valid_loss': 4.321889038646923, 'top1_accuracy': 0.5112271299205322, 'top5_accuracy': 0.7011873960450933} | TPU Validation: {'avg_valid_loss': 6.567002812553855, 'top1_accuracy': 0.32168268342265754, 'top5_accuracy': 0.532110515616337}]\n",
      "312it [03:36,  1.44it/s, Epoch 25 | GPU Loss: 0.113087 | TPU Loss: 0.204897 | GPU Validation: {'avg_valid_loss': 4.505056650498334, 'top1_accuracy': 0.4937165034189614, 'top5_accuracy': 0.6872112363703566} | TPU Validation: {'avg_valid_loss': 6.878153638278737, 'top1_accuracy': 0.3164849380890778, 'top5_accuracy': 0.5274209942709296}]\n",
      "312it [03:37,  1.44it/s, Epoch 26 | GPU Loss: 0.062824 | TPU Loss: 0.217972 | GPU Validation: {'avg_valid_loss': 4.414475785984712, 'top1_accuracy': 0.5079236739974127, 'top5_accuracy': 0.7008870818702643} | TPU Validation: {'avg_valid_loss': 6.98453442068661, 'top1_accuracy': 0.3185640362225097, 'top5_accuracy': 0.5264045462945851}]\n",
      "312it [03:38,  1.43it/s, Epoch 27 | GPU Loss: 0.079632 | TPU Loss: 0.205498 | GPU Validation: {'avg_valid_loss': 4.361774492263794, 'top1_accuracy': 0.5157780447237109, 'top5_accuracy': 0.7051838846793569} | TPU Validation: {'avg_valid_loss': 6.9300890642053945, 'top1_accuracy': 0.31941877656625395, 'top5_accuracy': 0.5316946959896507}]\n",
      "312it [03:38,  1.43it/s, Epoch 28 | GPU Loss: 0.106163 | TPU Loss: 0.174046 | GPU Validation: {'avg_valid_loss': 4.436773925669053, 'top1_accuracy': 0.5179495472186287, 'top5_accuracy': 0.7052300868600998} | TPU Validation: {'avg_valid_loss': 7.049313556446749, 'top1_accuracy': 0.3225143226760303, 'top5_accuracy': 0.5298697098503049}]\n",
      "312it [03:36,  1.44it/s, Epoch 29 | GPU Loss: 0.091451 | TPU Loss: 0.216752 | GPU Validation: {'avg_valid_loss': 4.2765499367433435, 'top1_accuracy': 0.5303086305673628, 'top5_accuracy': 0.7149787469968583} | TPU Validation: {'avg_valid_loss': 7.004304167803596, 'top1_accuracy': 0.3304148955830715, 'top5_accuracy': 0.5422980964701534}]\n",
      "312it [03:38,  1.43it/s, Epoch 30 | GPU Loss: 0.060320 | TPU Loss: 0.179351 | GPU Validation: {'avg_valid_loss': 4.434784754584817, 'top1_accuracy': 0.5158011458140824, 'top5_accuracy': 0.7048835705045278} | TPU Validation: {'avg_valid_loss': 7.02990042181576, 'top1_accuracy': 0.3380613564960266, 'top5_accuracy': 0.5421363888375531}]\n",
      "312it [03:37,  1.43it/s, Epoch 31 | GPU Loss: 0.071329 | TPU Loss: 0.164031 | GPU Validation: {'avg_valid_loss': 4.3624684165505805, 'top1_accuracy': 0.5220384402143781, 'top5_accuracy': 0.7119063019774533} | TPU Validation: {'avg_valid_loss': 7.034098670061897, 'top1_accuracy': 0.3413648124191462, 'top5_accuracy': 0.5489974126778784}]\n",
      "312it [03:39,  1.42it/s, Epoch 32 | GPU Loss: 0.093915 | TPU Loss: 0.205920 | GPU Validation: {'avg_valid_loss': 4.3989206987268785, 'top1_accuracy': 0.5276058029939014, 'top5_accuracy': 0.7121142117907965} | TPU Validation: {'avg_valid_loss': 7.009816955117619, 'top1_accuracy': 0.3506976529292183, 'top5_accuracy': 0.5600628349658104}]\n",
      "312it [03:39,  1.42it/s, Epoch 33 | GPU Loss: 0.073181 | TPU Loss: 0.214468 | GPU Validation: {'avg_valid_loss': 4.541747502719654, 'top1_accuracy': 0.5151774163740529, 'top5_accuracy': 0.7026427647384956} | TPU Validation: {'avg_valid_loss': 6.888139965954949, 'top1_accuracy': 0.3546710404731103, 'top5_accuracy': 0.5640593236000739}]\n",
      "312it [03:37,  1.44it/s, Epoch 34 | GPU Loss: 0.076260 | TPU Loss: 0.127572 | GPU Validation: {'avg_valid_loss': 4.428978028016932, 'top1_accuracy': 0.5280447237109591, 'top5_accuracy': 0.7119525041581962} | TPU Validation: {'avg_valid_loss': 6.9822799738715675, 'top1_accuracy': 0.35575679172056923, 'top5_accuracy': 0.5619802254666421}]\n",
      "312it [03:38,  1.42it/s, Epoch 35 | GPU Loss: 0.090736 | TPU Loss: 0.157216 | GPU Validation: {'avg_valid_loss': 4.30916885768666, 'top1_accuracy': 0.5408658288671225, 'top5_accuracy': 0.7211467381260396} | TPU Validation: {'avg_valid_loss': 7.007859981761259, 'top1_accuracy': 0.3486878580669008, 'top5_accuracy': 0.5559970430604324}]\n",
      "312it [03:39,  1.42it/s, Epoch 36 | GPU Loss: 0.107480 | TPU Loss: 0.105333 | GPU Validation: {'avg_valid_loss': 4.418032031900743, 'top1_accuracy': 0.5381861023840325, 'top5_accuracy': 0.7185825170948069} | TPU Validation: {'avg_valid_loss': 7.085665910384234, 'top1_accuracy': 0.35679634078728517, 'top5_accuracy': 0.5618416189244132}]\n",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from actors import get_tpu_actor, reset_all_actors, ActorResetter\n",
    "\n",
    "actor_resetter = ActorResetter(gpu_actor=gpu_actor, num_classes=NUM_CLASSES)\n",
    "gpu_actor, tpu_actor = reset_all_actors(gpu_actor, tpu_actor=None, actor_resetter=actor_resetter)\n",
    "ray.get(tpu_actor.set_matmul_precision.remote('default'))\n",
    "ray.get(gpu_actor.init_optimizer.remote(1e-3))\n",
    "ray.get(tpu_actor.init_optimizer.remote(1e-3))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "training_split = 'custom'\n",
    "\n",
    "gpu_losses = []\n",
    "tpu_losses = []\n",
    "gpu_validation_metrics = []\n",
    "tpu_validation_metrics = []\n",
    "for epoch in range(40):\n",
    "  gpu_stream = gpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  tpu_stream = tpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  progress = tqdm(zip(gpu_stream, tpu_stream, strict=True))\n",
    "  gpu_loss = None\n",
    "  tpu_loss = None\n",
    "  for gpu_stuff, tpu_stuff in progress:\n",
    "    gpu_stuff = ray.get(gpu_stuff)\n",
    "    tpu_stuff = ray.get(tpu_stuff)\n",
    "    if isinstance(gpu_stuff, tuple) and isinstance(tpu_stuff, tuple):\n",
    "      i, gpu_loss, gpu_data_hash = gpu_stuff\n",
    "      j, tpu_loss, tpu_data_hash = tpu_stuff\n",
    "      if i != j:\n",
    "        raise ValueError(\"Iteration numbers do not match between GPU and TPU actors!\")\n",
    "      if gpu_data_hash != tpu_data_hash:\n",
    "        raise ValueError(\"Data hashes do not match between GPU and TPU actors!\")\n",
    "      gpu_losses.append(gpu_loss)\n",
    "      tpu_losses.append(tpu_loss)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} Iter {i} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f}\")\n",
    "    elif isinstance(gpu_stuff, dict) and isinstance(tpu_stuff, dict):\n",
    "      gpu_validation_metrics.append(gpu_stuff)\n",
    "      tpu_validation_metrics.append(tpu_stuff)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f} | GPU Validation: {gpu_stuff} | TPU Validation: {tpu_stuff}\")\n",
    "    else:\n",
    "      raise ValueError(f\"Unexpected data types from GPU and TPU actors! {type(gpu_stuff)}, {type(tpu_stuff)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gpu_losses, label='GPU Loss')\n",
    "plt.plot(tpu_losses, label='TPU Loss')\n",
    "plt.xlabel('Iteration') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad961110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Save the gpu and tpu losses to `outputs/` in JSON format.\n",
    "os.makedirs(\"outputs/convergence/lr-1e-3\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/gpu_losses.json\", \"w\") as f:\n",
    "  json.dump(gpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/tpu_losses.json\", \"w\") as f:\n",
    "  json.dump(tpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/gpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(gpu_validation_metrics, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/tpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(tpu_validation_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985e719",
   "metadata": {},
   "source": [
    "### Appendix: test LR=1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ec757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "actor_resetter = ActorResetter(gpu_actor=gpu_actor, num_classes=NUM_CLASSES)\n",
    "gpu_actor, tpu_actor = reset_all_actors(gpu_actor, tpu_actor=None, actor_resetter=actor_resetter)\n",
    "ray.get(tpu_actor.set_matmul_precision.remote('default'))\n",
    "ray.get(gpu_actor.init_optimizer.remote(1e-4))\n",
    "ray.get(tpu_actor.init_optimizer.remote(1e-4))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "training_split = 'custom'\n",
    "\n",
    "gpu_losses = []\n",
    "tpu_losses = []\n",
    "gpu_validation_metrics = []\n",
    "tpu_validation_metrics = []\n",
    "for epoch in range(40):\n",
    "  gpu_stream = gpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  tpu_stream = tpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  progress = tqdm(zip(gpu_stream, tpu_stream, strict=True))\n",
    "  gpu_loss = None\n",
    "  tpu_loss = None\n",
    "  for gpu_stuff, tpu_stuff in progress:\n",
    "    gpu_stuff = ray.get(gpu_stuff)\n",
    "    tpu_stuff = ray.get(tpu_stuff)\n",
    "    if isinstance(gpu_stuff, tuple) and isinstance(tpu_stuff, tuple):\n",
    "      i, gpu_loss, gpu_data_hash = gpu_stuff\n",
    "      j, tpu_loss, tpu_data_hash = tpu_stuff\n",
    "      if i != j:\n",
    "        raise ValueError(\"Iteration numbers do not match between GPU and TPU actors!\")\n",
    "      if gpu_data_hash != tpu_data_hash:\n",
    "        raise ValueError(\"Data hashes do not match between GPU and TPU actors!\")\n",
    "      gpu_losses.append(gpu_loss)\n",
    "      tpu_losses.append(tpu_loss)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} Iter {i} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f}\")\n",
    "    elif isinstance(gpu_stuff, dict) and isinstance(tpu_stuff, dict):\n",
    "      gpu_validation_metrics.append(gpu_stuff)\n",
    "      tpu_validation_metrics.append(tpu_stuff)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f} | GPU Validation: {gpu_stuff} | TPU Validation: {tpu_stuff}\")\n",
    "    else:\n",
    "      raise ValueError(f\"Unexpected data types from GPU and TPU actors! {type(gpu_stuff)}, {type(tpu_stuff)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gpu_losses, label='GPU Loss')\n",
    "plt.plot(tpu_losses, label='TPU Loss')\n",
    "plt.xlabel('Iteration') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9334570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Save the gpu and tpu losses to `outputs/` in JSON format.\n",
    "os.makedirs(\"outputs/convergence/lr-1e-4\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/gpu_losses.json\", \"w\") as f:\n",
    "  json.dump(gpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/tpu_losses.json\", \"w\") as f:\n",
    "  json.dump(tpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/gpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(gpu_validation_metrics, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/tpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(tpu_validation_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5dc9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511aef3-178c-4980-a3f8-e26e072e7488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
