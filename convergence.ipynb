{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac118d15",
   "metadata": {},
   "source": [
    "# Prerequisite: start the cluster\n",
    "\n",
    "To use this notebook, we need an A100 VM and a TPU v5litepod-8 VM.\n",
    "Both VMs should be reachable via IP address to each other.\n",
    "This notebook is run on the A100 VM, which is also the ray head node.\n",
    "\n",
    "Starting the head node:\n",
    "\n",
    "```sh\n",
    "ray start --head --port=6379\n",
    "```\n",
    "\n",
    "Joining the ray cluster from the TPU VM:\n",
    "\n",
    "```sh\n",
    "ray start --address=$GPU_IP:6379\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21762670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 19:16:04,118\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.128.0.32:6379...\n",
      "2025-04-30 19:16:04,176\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7676c94acee045b98578e2fb33a88da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.10.14</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.44.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.10.14', ray_version='2.44.1', ray_commit='daca7b2b1a950dc7f731e34e74c76ae383794ffe')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TpuResNetWorker pid=1674225, ip=10.138.0.2)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/30 12:17:16\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.764818 1674225 b295d63588a.cc:733] Linux version 6.5.0-1013-gcp (buildd@lcy02-amd64-064) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #13~22.04.1-Ubuntu SMP Wed Jan 24 23:39:40 UTC 2024\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774626 1674225 b295d63588a.cc:815] Process id 1674225\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774643 1674225 b295d63588a.cc:820] Current working directory /home/yifeit_google_com/resnet18\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774645 1674225 b295d63588a.cc:822] Current timezone is PDT (currently UTC -07:00)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774648 1674225 b295d63588a.cc:826] Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774649 1674225 b295d63588a.cc:827]  at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774650 1674225 b295d63588a.cc:828]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774651 1674225 b295d63588a.cc:829]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774652 1674225 b295d63588a.cc:832]  from changelist 719471581 with baseline 706826864 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/706826864.1/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774654 1674225 b295d63588a.cc:836] Build label: libtpu_lts_20241216_c_RC02\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774655 1674225 b295d63588a.cc:838] Build tool: Bazel, release r4rca-2024.12.08-1 (mainline @703883733)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774656 1674225 b295d63588a.cc:839] Build target: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774657 1674225 b295d63588a.cc:846] Command line arguments:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774657 1674225 b295d63588a.cc:848] argv[0]: './tpu_driver'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774660 1674225 b295d63588a.cc:848] argv[1]: '--minloglevel=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774661 1674225 b295d63588a.cc:848] argv[2]: '--stderrthreshold=3'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774663 1674225 b295d63588a.cc:848] argv[3]: '--v=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774664 1674225 b295d63588a.cc:848] argv[4]: '--vmodule='\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774665 1674225 b295d63588a.cc:848] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774667 1674225 b295d63588a.cc:848] argv[6]: '--max_log_size=1024'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774668 1674225 b295d63588a.cc:848] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774669 1674225 b295d63588a.cc:848] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774671 1674225 b295d63588a.cc:848] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774672 1674225 b295d63588a.cc:848] argv[10]: '--2a886c8_chips_per_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774673 1674225 b295d63588a.cc:848] argv[11]: '--2a886c8_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774674 1674225 b295d63588a.cc:848] argv[12]: '--2a886c8_hal_included_devs=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774676 1674225 b295d63588a.cc:848] argv[13]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774677 1674225 b295d63588a.cc:848] argv[14]: '--2a886c8_slice_builder_worker_addresses=10.138.0.2:8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774678 1674225 b295d63588a.cc:848] argv[15]: '--tpu_slice_builder_dump_chip=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774679 1674225 b295d63588a.cc:848] argv[16]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774681 1674225 b295d63588a.cc:848] argv[17]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774682 1674225 b295d63588a.cc:848] argv[18]: '--runtime_metric_service_port=8431'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774683 1674225 b295d63588a.cc:848] argv[19]: '--tpu_hbm_report_enable=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774685 1674225 b295d63588a.cc:848] argv[20]: '--tpu_hbm_report_frequency=5s'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774686 1674225 b295d63588a.cc:848] argv[21]: '--enable_runtime_uptime_telemetry=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774687 1674225 b295d63588a.cc:848] argv[22]: ''\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774688 1674225 b295d63588a.cc:848] argv[23]: '--xla_latency_hiding_scheduler_rerun=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774690 1674225 b295d63588a.cc:848] argv[24]: '--xla_tpu_prefer_async_allgather_to_allreduce=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774691 1674225 b295d63588a.cc:848] argv[25]: '--xla_tpu_enable_flash_attention=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774692 1674225 b295d63588a.cc:848] argv[26]: '--xla_enable_async_all_gather=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774693 1674225 b295d63588a.cc:848] argv[27]: '--xla_enable_async_collective_permute=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.774996 1674225 init.cc:78] Remote crash gathering hook installed.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.775024 1674225 tpu_runtime_type_flags.cc:91] --tpu_use_tfrt not specified. Using default value: true\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.781529 1674225 tf_tpu_flags.cc:60] 2a886c8Platform is NOT registered.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.781714 1674225 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.781839 1674225 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.911147 1674225 config.cc:256] gRPC experiments enabled: max_pings_wo_data_throttle, monitoring_experiment, pick_first_new, time_caching_in_party, trace_record_callops, work_serializer_dispatch\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.920599 1674225 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 246, prefix = futex-default\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.921040 1674225 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <0A-00 00-00 00-00 00-00 80-30 82-7C 3D-56 00-00 80-A5 39-B7 77-7F 00-00 58-95 A1-B8 77-7F 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = (nil), clock = 0x563d7ba93438\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.956122 1674225 singleton_tpu_states_manager.cc:72] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.956132 1674225 singleton_tpu_states_manager.cc:95] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.959637 1674225 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.959644 1674225 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.959649 1674225 tpu_version_flag.cc:53] Using auto-detected TPU version TPU v5 lite\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.962773 1674225 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.962778 1674225 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.965829 1674225 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.965834 1674225 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.968934 1674225 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.968939 1674225 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.975120 1674997 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.975149 1674997 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.975165 1674997 flags_util.cc:314] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.978246 1674997 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.978251 1674997 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:16.978257 1674997 tpu_network_factory.cc:62] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:17.324488 1674999 async_driver.cc:427] [/dev/vfio/0 tpu9:pe1:1] Driver opened.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:17.356670 1674997 slice_builder_helper.cc:98] Current host is used as SliceBuilder master.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:17.356974 1674997 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:17.369410 1674997 master.cc:221] Successfully initialized SliceBuilder master session cb5ba6aed4514c38 with expected topology (1, 1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:17.369621 1674997 tpu_hal.cc:198] Starting premapped memory manager initialization...\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.089202 1674997 runtime_metric_service.cc:122] Successfully started Runtime Metric Service on port: 8431\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.089346 1674997 system.cc:1053] tpu::System initialized, current host id: 0, logical device ids: 0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.089369 1674225 tfrt_tpu_system_state.cc:213] CreateTpuSystemState: TPU initialization is successful and it took 3.11798998s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.089381 1674225 tfrt_tpu_system_state.cc:217] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.089399 1674225 tpu_host_allocator.cc:39] Premapped buffer is using alignment 512\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.089576 1674225 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.317497 1674225 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 21 instructions, modules: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.317525 1674225 autofdo_agent.cc:203] xla_tpu_autofdo_profile_dir updated to \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:17:20.317526 1674225 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/30 12:17:20\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:17:20.317526 1674225 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.317535 1674225 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.22 instructions: 21 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.318297 1674225 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.22 instructions: 142\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.318302 1674225 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.321831 1674225 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.323369 1674225 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 7.67312425ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.481376 1675303 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly    \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.487935 1674998 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 7.70576525ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.489860 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 457 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.492428 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494139 1674998 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (52.0K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494497 1674998 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494500 1674998 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 52.0K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494503 1674998 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 8.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494514 1674998 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.05M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494515 1674998 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494516 1674998 2a886c8_compiler_base.cc:3101]     program           52.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494517 1674998 2a886c8_compiler_base.cc:3101]     arguments            0B \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494517 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494518 1674998 2a886c8_compiler_base.cc:3101] Output size 20.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494519 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494526 1674998 2a886c8_compiler_base.cc:3105] Program sflag requirement 212B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494526 1674998 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494527 1674998 2a886c8_compiler_base.cc:3105]     scoped               8B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494528 1674998 2a886c8_compiler_base.cc:3105] Program vmem requirement 8.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494528 1674998 2a886c8_compiler_base.cc:3105]     scoped             8.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494529 1674998 2a886c8_compiler_base.cc:3105] Program smem requirement 592B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494530 1674998 2a886c8_compiler_base.cc:3105]     scoped             592B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494530 1674998 2a886c8_compiler_base.cc:3105] Program hbm requirement 52.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494531 1674998 2a886c8_compiler_base.cc:3105]     overlays          52.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494532 1674998 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (0 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494544 1674998 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 6.54454325ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494632 1674998 isa_program.cc:328] (HLO module SyncTensorsGraph.22): Executable fingerprint:c0190226e36366352ca22d7e2e9151366912c050fdf35b82d07ab1b113349f2d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494634 1674998 isa_program.cc:332] (HLO module SyncTensorsGraph.22): Executable fingerprint (including data segments):89cba99eca55ab4a5f0d76ec4b1e480653cb821a975fbc8b884f46e30673494a\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494635 1674998 isa_program.cc:335] (HLO module SyncTensorsGraph.22): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:20.494858 1674225 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 179.282538ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.194046 1674225 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 2 instructions, modules: SyncTensorsGraph.3\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.194062 1674225 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.3 instruc\n",
      "\u001b[36m(GpuResNetWorker pid=2737858)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(TpuResNetWorker pid=1674225, ip=10.138.0.2)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(TpuResNetWorker pid=1674225, ip=10.138.0.2)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(TpuResNetWorker pid=1674225, ip=10.138.0.2)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m tions: 2 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.194307 1674225 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.3 instructions: 9\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.194310 1674225 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.196613 1674225 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.197213 1674225 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 4.31997825ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.197818 1675305 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly    \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.201721 1674998 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 4.17246775ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.202296 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 133 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.204672 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205349 1674998 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (31.5K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205515 1674998 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.3\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205517 1674998 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 31.5K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205521 1674998 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 8.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205529 1674998 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.03M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205530 1674998 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205531 1674998 2a886c8_compiler_base.cc:3101]     program           31.5K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205532 1674998 2a886c8_compiler_base.cc:3101]     arguments            0B \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205532 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205533 1674998 2a886c8_compiler_base.cc:3101] Output size 1.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205534 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205545 1674998 2a886c8_compiler_base.cc:3105] Program sflag requirement 208B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205546 1674998 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205547 1674998 2a886c8_compiler_base.cc:3105]     scoped               4B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205548 1674998 2a886c8_compiler_base.cc:3105] Program vmem requirement 8.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205548 1674998 2a886c8_compiler_base.cc:3105]     scoped             8.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205549 1674998 2a886c8_compiler_base.cc:3105] Program smem requirement 512B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205550 1674998 2a886c8_compiler_base.cc:3105]     scoped             512B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205550 1674998 2a886c8_compiler_base.cc:3105] Program hbm requirement 31.5K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205551 1674998 2a886c8_compiler_base.cc:3105]     overlays          31.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205552 1674998 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (0 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205560 1674998 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 3.78709725ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205612 1674998 isa_program.cc:328] (HLO module SyncTensorsGraph.3): Executable fingerprint:ce6541084e6355686346fb3430e98c421aacd1ac6afe69b6b0af81547ecd070e\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205614 1674998 isa_program.cc:332] (HLO module SyncTensorsGraph.3): Executable fingerprint (including data segments):53292d34f1aaf3df4cdae80daa103f78473d81c0ce60a51e1354c4ab008325d1\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205616 1674998 isa_program.cc:335] (HLO module SyncTensorsGraph.3): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:17:22.205732 1674225 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 12.8931245ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:08.501079 1674225 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 530 instructions, modules: SyncTensorsGraph.542\n",
      "\u001b[36m(TpuResNetWorker pid=1674225, ip=10.138.0.2)\u001b[0m Using cached datasets.\n",
      "\u001b[36m(GpuResNetWorker pid=2737858)\u001b[0m Number of classes: 10177\n",
      "\u001b[36m(GpuResNetWorker pid=2737858)\u001b[0m Train set size: 159267\n",
      "\u001b[36m(GpuResNetWorker pid=2737858)\u001b[0m Test set size: 43288\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:08.501103 1674225 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.542 instructions: 530 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:08.505701 1674225 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.542 instructions: 1,010\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:08.505706 1674225 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:08.604665 1674225 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:08.661593 1674225 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 161.6333535ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:08.679097 1675305 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly    \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:15.752677 1674998 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 7.0835620805s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.016316 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 193,035 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.029860 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.107513 1674998 2a886c8_compiler_base.cc:2869] Program divided into 13 overlays without HLO functions (11.82M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222880 1674998 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.542\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222896 1674998 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 1.93G / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222905 1674998 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 106.33M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222916 1674998 2a886c8_compiler_base.cc:3101] Total hbm usage >= 2.53G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222917 1674998 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222918 1674998 2a886c8_compiler_base.cc:3101]     program           1.93G \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222918 1674998 2a886c8_compiler_base.cc:3101]     arguments       357.20M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222919 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222920 1674998 2a886c8_compiler_base.cc:3101] Output size 1.0K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222920 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222936 1674998 2a886c8_compiler_base.cc:3105] Program sflag requirement 332B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222936 1674998 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222937 1674998 2a886c8_compiler_base.cc:3105]     scoped              76B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222938 1674998 2a886c8_compiler_base.cc:3105]     HLO temp            52B (100.0% utilization: Unpadded (52B) Padded (52B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222939 1674998 2a886c8_compiler_base.cc:3105] Program hbm requirement 1.93G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222939 1674998 2a886c8_compiler_base.cc:3105]     HLO temp          1.92G (100.0% utilization: Unpadded (1.91G) Padded (1.91G), 0.1% fragmentation (2.16M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222940 1674998 2a886c8_compiler_base.cc:3105]     overlays         11.82M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222940 1674998 2a886c8_compiler_base.cc:3105] Program vmem requirement 106.33M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222941 1674998 2a886c8_compiler_base.cc:3105]     scoped           14.93M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222942 1674998 2a886c8_compiler_base.cc:3105]     HLO temp         91.40M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (91.40M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222942 1674998 2a886c8_compiler_base.cc:3105] Program smem requirement 516B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222943 1674998 2a886c8_compiler_base.cc:3105]     scoped             516B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222944 1674998 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.8K / 1.00M (104 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.222966 1674998 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 470.223002ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.231781 1674998 isa_program.cc:328] (HLO module SyncTensorsGraph.542): Executable fingerprint:ab83cdcf3f4644c3d75b5806f593cf55e30394d5520f8d5056473b50d8b77fde\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.231789 1674998 isa_program.cc:332] (HLO module SyncTensorsGraph.542): Executable fingerprint (including data segments):d692b496de3ebaf0d0415d30b82d0df76f2f8c211d895752279ac70a6c8863d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(GpuResNetWorker pid=2737858)\u001b[0m /pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m 1\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.231791 1674998 isa_program.cc:335] (HLO module SyncTensorsGraph.542): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.316916 1674225 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 7.817732254s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.528470 1674225 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 3,592 instructions, modules: SyncTensorsGraph.3627\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.528485 1674225 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.3627 instructions: 3,592 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.553954 1674225 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.3627 instructions: 6,321\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:16.553962 1674225 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:17.033182 1674225 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:17.404525 1674225 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 877.407582ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:17.479985 1675310 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly    \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:25.675381 1674998 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 8.24080059475s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:26.603777 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 582,181 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:26.639239 1674998 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:26.830106 1674998 2a886c8_compiler_base.cc:2869] Program divided into 33 overlays without HLO functions (35.59M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178937 1674998 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.3627\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178953 1674998 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 8.13G / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178963 1674998 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 127.08M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178973 1674998 2a886c8_compiler_base.cc:3101] Total hbm usage >= 8.73G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178975 1674998 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178976 1674998 2a886c8_compiler_base.cc:3101]     program           8.13G \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178976 1674998 2a886c8_compiler_base.cc:3101]     arguments       357.23M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178977 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178977 1674998 2a886c8_compiler_base.cc:3101] Output size 292.49M; shares 63.22M with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.178978 1674998 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179003 1674998 2a886c8_compiler_base.cc:3105] Program sflag requirement 364B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179003 1674998 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179004 1674998 2a886c8_compiler_base.cc:3105]     scoped              76B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179005 1674998 2a886c8_compiler_base.cc:3105]     HLO temp            84B (100.0% utilization: Unpadded (80B) Padded (80B), 4.8% fragmentation (4B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179005 1674998 2a886c8_compiler_base.cc:3105] Program hbm requirement 8.13G:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179006 1674998 2a886c8_compiler_base.cc:3105]     HLO temp          8.10G (100.0% utilization: Unpadded (8.09G) Padded (8.09G), 0.1% fragmentation (11.11M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179007 1674998 2a886c8_compiler_base.cc:3105]     overlays         35.59M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179008 1674998 2a886c8_compiler_base.cc:3105] Program vmem requirement 127.08M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179008 1674998 2a886c8_compiler_base.cc:3105]     scoped           15.56M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179009 1674998 2a886c8_compiler_base.cc:3105]     HLO temp        111.53M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (111.53M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179009 1674998 2a886c8_compiler_base.cc:3105] Program smem requirement 1.9K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179010 1674998 2a886c8_compiler_base.cc:3105]     scoped             1.9K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179011 1674998 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 4.3K / 1.00M (131 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.179034 1674998 2a886c8_c\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m ompiler_base.cc:3135] CODE_GENERATION stage duration: 1.50338075625s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.208230 1674998 isa_program.cc:328] (HLO module SyncTensorsGraph.3627): Executable fingerprint:214eb50ad7ea073c54a091929d98f8925f72c50ca4ee1c365dba66782b861dc3\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.208237 1674998 isa_program.cc:332] (HLO module SyncTensorsGraph.3627): Executable fingerprint (including data segments):8114e4493b840de820c9b9a413c3d395fdbadbf380e44059283574afb1b8d765\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.208239 1674998 isa_program.cc:335] (HLO module SyncTensorsGraph.3627): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:27.567551 1674225 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 11.0453370845s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:28.557400 1674225 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 3,097 instructions, modules: SyncTensorsGraph.3132\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:28.557416 1674225 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.3132 instructions: 3,097 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:28.582800 1674225 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.3132 instructions: 5,826\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:28.582807 1674225 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:29.135197 1674225 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:29.556626 1674225 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 1.000714101s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0430 12:18:29.641292 1679924 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly    \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:18:29.987296 1679922 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:18:29.987296 1679922 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:18:30.016449 1679922 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0430 12:18:30.016449 1679922 llo_loop.cc:101] [copy.236] 0-iteration loop inserted, body will not be executed: start=0, step=1, limit=0\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1e4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision import datasets, models, tv_tensors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962db9a",
   "metadata": {},
   "source": [
    "## Look at our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584152da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a486a47d68f46239fbca14fbf4b330e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90c6a1c50054746918083f9ea79bf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c550abab6041529196984d6b3ee1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e740149ac75b40d0bbd0a4527d010f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb2954f142748c59ea899923fab0287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0daa285f794cdc92350367c7467c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c75b554d3e2406aac122f92a6d27c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_raw_dataset(split):\n",
    "  dataset = load_dataset(\"flwrlabs/celeba\", split=split, trust_remote_code=True, cache_dir='cache')\n",
    "  return dataset.with_format(\"torch\")\n",
    "\n",
    "train = load_raw_dataset(\"train\")\n",
    "test = load_raw_dataset(\"test\")\n",
    "valid = load_raw_dataset(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12ff363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 8192\n",
      "Test set: 1000\n",
      "Valid set: 985\n",
      "Total unique IDs: 10177\n"
     ]
    }
   ],
   "source": [
    "train_set = train.unique('celeb_id') # type: ignore\n",
    "test_set = test.unique('celeb_id') # type: ignore\n",
    "valid_set = valid.unique('celeb_id') # type: ignore\n",
    "print(f\"Train set: {len(train_set)}\")\n",
    "print(f\"Test set: {len(test_set)}\")\n",
    "print(f\"Valid set: {len(valid_set)}\")\n",
    "print(f\"Total unique IDs: {len(set(train_set) | set(test_set) | set(valid_set))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6aee5",
   "metadata": {},
   "source": [
    "Problem: there is no label overlap between train/test/valid sets.\n",
    "\n",
    "We can't use the validation set to validate accuracy because it contains\n",
    "completely unseen labels.\n",
    "\n",
    "For the purpose of this specific convergence test, we'll do our own split\n",
    "such that the same labels show up in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d59388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached datasets.\n",
      "Number of classes: 10177\n",
      "Train set size: 159267\n",
      "Test set size: 43288\n"
     ]
    }
   ],
   "source": [
    "from data_util import download_dataset_custom_split_cached\n",
    "\n",
    "train_dataset, test_dataset, NUM_CLASSES = download_dataset_custom_split_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28dd816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2489/2489 [00:39<00:00, 63.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in train loader: 10133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 677/677 [00:10<00:00, 66.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in test loader: 10133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8, shuffle=False)  # type: ignore\n",
    "\n",
    "# Enumerate train loader to find number of unique labels\n",
    "train_unique_labels = set()\n",
    "for batch in tqdm(train_loader):\n",
    "    labels = batch['label']\n",
    "    train_unique_labels.update(set(labels.flatten().numpy().tolist()))\n",
    "print(f\"Number of unique labels in train loader: {len(train_unique_labels)}\")\n",
    "# Enumerate test loader to find number of unique labels\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=8, shuffle=False)  # type: ignore\n",
    "test_unique_labels = set()\n",
    "for batch in tqdm(test_loader):\n",
    "    labels = batch['label']\n",
    "    test_unique_labels.update(set(labels.flatten().numpy().tolist()))\n",
    "print(f\"Number of unique labels in test loader: {len(test_unique_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf0c841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10176\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = max(train_unique_labels)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "assert train_unique_labels == test_unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378f904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176],\n",
       " [10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(train_unique_labels))[-10:], sorted(list(test_unique_labels))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b473163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Actor (PID: 2737709) using cuda (NVIDIA A100-SXM4-80GB)\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "\n",
      "/home/yifeit_google_com/resnet18\n"
     ]
    }
   ],
   "source": [
    "from actors import get_gpu_actor\n",
    "\n",
    "gpu_actor = get_gpu_actor()\n",
    "print(ray.get(gpu_actor.print_model_architecture.remote()))\n",
    "print(ray.get(gpu_actor.get_cwd.remote()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba9c39",
   "metadata": {},
   "source": [
    "## Convergence test\n",
    "\n",
    "Now let's run a proper training run with:\n",
    "\n",
    "- Learning rate: `1e-3` (unchanged)\n",
    "- Batch size: 512 (unchanged)\n",
    "- Training set: training subset of the remixed `flwrlabs/celeba`\n",
    "- Validation set (used to compute validation accuracy): testing subset of the remixed `flwrlabs/celeba`\n",
    "- Measure:\n",
    "  - val-loss\n",
    "  - val-top1-acc\n",
    "\n",
    "If TPU is materially worse on val-top1-acc with same LR/batchsize, that's a\n",
    "material datapoint and we probably want to bring it back to the TPU team to see\n",
    "if that's also expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Actor (PID: 2737858) using cuda (NVIDIA A100-SXM4-80GB)\n",
      "TPU Actor (PID: 1674225) using xla:0 (v5litepod-8)\n",
      " GPU and TPU models have identical state dicts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:04, 64.71s/it, Epoch 0 Iter 0 | GPU Loss: 9.429099 | TPU Loss: 9.429420]2025-04-30 19:18:28,223\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::GpuResNetWorker.run_training_epoch()\u001b[39m (pid=2737858, ip=10.128.0.32, actor_id=855f688340cd3e7b6db770a92b000000, repr=<actors.GpuResNetWorker object at 0x7f02a33c2e30>)\n",
      "    loss, _grad_dict = self.run_training_pass(images, labels)\n",
      "  File \"/home/yifeit_google_com/resnet18/actors.py\", line 93, in run_training_pass\n",
      "    loss_float = loss.detach().cpu().item()\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from actors import get_tpu_actor, reset_all_actors, ActorResetter\n",
    "\n",
    "actor_resetter = ActorResetter(gpu_actor=gpu_actor, num_classes=NUM_CLASSES)\n",
    "gpu_actor, tpu_actor = reset_all_actors(gpu_actor, tpu_actor=None, actor_resetter=actor_resetter)\n",
    "ray.get(tpu_actor.set_matmul_precision.remote('default'))\n",
    "ray.get(gpu_actor.init_optimizer.remote(1e-3))\n",
    "ray.get(tpu_actor.init_optimizer.remote(1e-3))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "training_split = 'custom'\n",
    "\n",
    "gpu_losses = []\n",
    "tpu_losses = []\n",
    "gpu_validation_metrics = []\n",
    "tpu_validation_metrics = []\n",
    "for epoch in range(40):\n",
    "  gpu_stream = gpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  tpu_stream = tpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  progress = tqdm(zip(gpu_stream, tpu_stream, strict=True))\n",
    "  gpu_loss = None\n",
    "  tpu_loss = None\n",
    "  for gpu_stuff, tpu_stuff in progress:\n",
    "    gpu_stuff = ray.get(gpu_stuff)\n",
    "    tpu_stuff = ray.get(tpu_stuff)\n",
    "    if isinstance(gpu_stuff, tuple) and isinstance(tpu_stuff, tuple):\n",
    "      i, gpu_loss, gpu_data_hash = gpu_stuff\n",
    "      j, tpu_loss, tpu_data_hash = tpu_stuff\n",
    "      if i != j:\n",
    "        raise ValueError(\"Iteration numbers do not match between GPU and TPU actors!\")\n",
    "      if gpu_data_hash != tpu_data_hash:\n",
    "        raise ValueError(\"Data hashes do not match between GPU and TPU actors!\")\n",
    "      gpu_losses.append(gpu_loss)\n",
    "      tpu_losses.append(tpu_loss)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} Iter {i} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f}\")\n",
    "    elif isinstance(gpu_stuff, dict) and isinstance(tpu_stuff, dict):\n",
    "      gpu_validation_metrics.append(gpu_stuff)\n",
    "      tpu_validation_metrics.append(tpu_stuff)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f} | GPU Validation: {gpu_stuff} | TPU Validation: {tpu_stuff}\")\n",
    "    else:\n",
    "      raise ValueError(f\"Unexpected data types from GPU and TPU actors! {type(gpu_stuff)}, {type(tpu_stuff)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gpu_losses, label='GPU Loss')\n",
    "plt.plot(tpu_losses, label='TPU Loss')\n",
    "plt.xlabel('Iteration') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad961110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Save the gpu and tpu losses to `outputs/` in JSON format.\n",
    "os.makedirs(\"outputs/convergence/lr-1e-3\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/gpu_losses.json\", \"w\") as f:\n",
    "  json.dump(gpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/tpu_losses.json\", \"w\") as f:\n",
    "  json.dump(tpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/gpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(gpu_validation_metrics, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-3/tpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(tpu_validation_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985e719",
   "metadata": {},
   "source": [
    "### Appendix: test LR=1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ec757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "actor_resetter = ActorResetter(gpu_actor=gpu_actor, num_classes=NUM_CLASSES)\n",
    "gpu_actor, tpu_actor = reset_all_actors(gpu_actor, tpu_actor=None, actor_resetter=actor_resetter)\n",
    "ray.get(tpu_actor.set_matmul_precision.remote('default'))\n",
    "ray.get(gpu_actor.init_optimizer.remote(1e-4))\n",
    "ray.get(tpu_actor.init_optimizer.remote(1e-4))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "training_split = 'custom'\n",
    "\n",
    "gpu_losses = []\n",
    "tpu_losses = []\n",
    "gpu_validation_metrics = []\n",
    "tpu_validation_metrics = []\n",
    "for epoch in range(40):\n",
    "  gpu_stream = gpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  tpu_stream = tpu_actor.run_training_epoch.remote(batch_size=BATCH_SIZE, training_split=training_split, shuffle=True, run_validation=True, dataloader_seed=epoch)\n",
    "  progress = tqdm(zip(gpu_stream, tpu_stream, strict=True))\n",
    "  gpu_loss = None\n",
    "  tpu_loss = None\n",
    "  for gpu_stuff, tpu_stuff in progress:\n",
    "    gpu_stuff = ray.get(gpu_stuff)\n",
    "    tpu_stuff = ray.get(tpu_stuff)\n",
    "    if isinstance(gpu_stuff, tuple) and isinstance(tpu_stuff, tuple):\n",
    "      i, gpu_loss, gpu_data_hash = gpu_stuff\n",
    "      j, tpu_loss, tpu_data_hash = tpu_stuff\n",
    "      if i != j:\n",
    "        raise ValueError(\"Iteration numbers do not match between GPU and TPU actors!\")\n",
    "      if gpu_data_hash != tpu_data_hash:\n",
    "        raise ValueError(\"Data hashes do not match between GPU and TPU actors!\")\n",
    "      gpu_losses.append(gpu_loss)\n",
    "      tpu_losses.append(tpu_loss)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} Iter {i} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f}\")\n",
    "    elif isinstance(gpu_stuff, dict) and isinstance(tpu_stuff, dict):\n",
    "      gpu_validation_metrics.append(gpu_stuff)\n",
    "      tpu_validation_metrics.append(tpu_stuff)\n",
    "      progress.set_postfix_str(f\"Epoch {epoch} | GPU Loss: {gpu_loss:.6f} | TPU Loss: {tpu_loss:.6f} | GPU Validation: {gpu_stuff} | TPU Validation: {tpu_stuff}\")\n",
    "    else:\n",
    "      raise ValueError(f\"Unexpected data types from GPU and TPU actors! {type(gpu_stuff)}, {type(tpu_stuff)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gpu_losses, label='GPU Loss')\n",
    "plt.plot(tpu_losses, label='TPU Loss')\n",
    "plt.xlabel('Iteration') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9334570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Save the gpu and tpu losses to `outputs/` in JSON format.\n",
    "os.makedirs(\"outputs/convergence/lr-1e-4\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/gpu_losses.json\", \"w\") as f:\n",
    "  json.dump(gpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/tpu_losses.json\", \"w\") as f:\n",
    "  json.dump(tpu_losses, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/gpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(gpu_validation_metrics, f)\n",
    "\n",
    "with open(\"outputs/convergence/lr-1e-4/tpu_validation_metrics.json\", \"w\") as f:\n",
    "  json.dump(tpu_validation_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5dc9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511aef3-178c-4980-a3f8-e26e072e7488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
