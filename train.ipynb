{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21762670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 03:33:59,506\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.128.0.32:6379...\n",
      "2025-04-15 03:33:59,520\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141e65e3d1254079b327e6917d4eec28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.10.14</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.44.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.10.14', ray_version='2.44.1', ray_commit='daca7b2b1a950dc7f731e34e74c76ae383794ffe')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(GpuResNetWorker pid=442604)\u001b[0m Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/yifeit_google_com/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "  0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      " 22%|██▏       | 9.62M/44.7M [00:00<00:00, 101MB/s]\n",
      " 69%|██████▊   | 30.6M/44.7M [00:00<00:00, 170MB/s]\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 175MB/s]\n",
      "\u001b[36m(TpuResNetWorker pid=3507724, ip=10.138.0.2)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 20:55:46\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.737246 3507724 b295d63588a.cc:733] Linux version 6.5.0-1013-gcp (buildd@lcy02-amd64-064) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #13~22.04.1-Ubuntu SMP Wed Jan 24 23:39:40 UTC 2024\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747436 3507724 b295d63588a.cc:815] Process id 3507724\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747453 3507724 b295d63588a.cc:820] Current working directory /home/yifeit_google_com/resnet18\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747454 3507724 b295d63588a.cc:822] Current timezone is PDT (currently UTC -07:00)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747457 3507724 b295d63588a.cc:826] Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747458 3507724 b295d63588a.cc:827]  at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747459 3507724 b295d63588a.cc:828]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747460 3507724 b295d63588a.cc:829]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747462 3507724 b295d63588a.cc:832]  from changelist 719471581 with baseline 706826864 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/706826864.1/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747463 3507724 b295d63588a.cc:836] Build label: libtpu_lts_20241216_c_RC02\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747464 3507724 b295d63588a.cc:838] Build tool: Bazel, release r4rca-2024.12.08-1 (mainline @703883733)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747465 3507724 b295d63588a.cc:839] Build target: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747466 3507724 b295d63588a.cc:846] Command line arguments:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747467 3507724 b295d63588a.cc:848] argv[0]: './tpu_driver'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747469 3507724 b295d63588a.cc:848] argv[1]: '--minloglevel=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747471 3507724 b295d63588a.cc:848] argv[2]: '--stderrthreshold=3'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747472 3507724 b295d63588a.cc:848] argv[3]: '--v=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747473 3507724 b295d63588a.cc:848] argv[4]: '--vmodule='\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747475 3507724 b295d63588a.cc:848] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747476 3507724 b295d63588a.cc:848] argv[6]: '--max_log_size=1024'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747477 3507724 b295d63588a.cc:848] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747478 3507724 b295d63588a.cc:848] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747480 3507724 b295d63588a.cc:848] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747481 3507724 b295d63588a.cc:848] argv[10]: '--2a886c8_chips_per_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747482 3507724 b295d63588a.cc:848] argv[11]: '--2a886c8_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747484 3507724 b295d63588a.cc:848] argv[12]: '--2a886c8_hal_included_devs=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747485 3507724 b295d63588a.cc:848] argv[13]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747486 3507724 b295d63588a.cc:848] argv[14]: '--2a886c8_slice_builder_worker_addresses=10.138.0.2:8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747488 3507724 b295d63588a.cc:848] argv[15]: '--tpu_slice_builder_dump_chip=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747489 3507724 b295d63588a.cc:848] argv[16]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747490 3507724 b295d63588a.cc:848] argv[17]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747491 3507724 b295d63588a.cc:848] argv[18]: '--runtime_metric_service_port=8431'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747493 3507724 b295d63588a.cc:848] argv[19]: '--tpu_hbm_report_enable=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747494 3507724 b295d63588a.cc:848] argv[20]: '--tpu_hbm_report_frequency=5s'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747495 3507724 b295d63588a.cc:848] argv[21]: '--enable_runtime_uptime_telemetry=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747497 3507724 b295d63588a.cc:848] argv[22]: ''\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747498 3507724 b295d63588a.cc:848] argv[23]: '--xla_latency_hiding_scheduler_rerun=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747499 3507724 b295d63588a.cc:848] argv[24]: '--xla_tpu_prefer_async_allgather_to_allreduce=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747500 3507724 b295d63588a.cc:848] argv[25]: '--xla_tpu_enable_flash_attention=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747502 3507724 b295d63588a.cc:848] argv[26]: '--xla_enable_async_all_gather=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747503 3507724 b295d63588a.cc:848] argv[27]: '--xla_enable_async_collective_permute=true'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TpuResNetWorker pid=3507724, ip=10.138.0.2)\u001b[0m Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/yifeit_google_com/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "  0%|          | 0.00/44.7M [00:00<?, ?B/s]8.0.2)\u001b[0m \n",
      " 43%|████▎     | 19.4M/44.7M [00:00<00:00, 203MB/s]0m \n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 257MB/s]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747841 3507724 init.cc:78] Remote crash gathering hook installed.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.747870 3507724 tpu_runtime_type_flags.cc:91] --tpu_use_tfrt not specified. Using default value: true\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.754374 3507724 tf_tpu_flags.cc:60] 2a886c8Platform is NOT registered.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.754579 3507724 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.754825 3507724 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.855734 3507724 config.cc:256] gRPC experiments enabled: max_pings_wo_data_throttle, monitoring_experiment, pick_first_new, time_caching_in_party, trace_record_callops, work_serializer_dispatch\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.864966 3507724 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 246, prefix = futex-default\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.865241 3507724 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <0A-00 00-00 00-00 00-00 00-69 1A-30 F7-55 00-00 80-A5 B9-F1 FF-7E 00-00 58-95 21-F3 FF-7E 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = (nil), clock = 0x55f72f3a3488\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.914163 3507724 singleton_tpu_states_manager.cc:72] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.914174 3507724 singleton_tpu_states_manager.cc:95] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.917732 3507724 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.917739 3507724 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.917743 3507724 tpu_version_flag.cc:53] Using auto-detected TPU version TPU v5 lite\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.920852 3507724 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.920857 3507724 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.923997 3507724 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.924003 3507724 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.927140 3507724 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.927145 3507724 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.931933 3508549 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.931959 3508549 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.931974 3508549 flags_util.cc:314] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.935081 3508549 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.935087 3508549 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:46.935092 3508549 tpu_network_factory.cc:62] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:47.288053 3508552 async_driver.cc:427] [/dev/vfio/0 tpu9:pe1:1] Driver opened.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:47.321630 3508549 slice_builder_helper.cc:98] Current host is used as SliceBuilder master.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:47.321982 3508549 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:47.335297 3508549 master.cc:221] Successfully initialized SliceBuilder master session 25e6fb256112f3ed with expected topology (1, 1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:47.335565 3508549 tpu_hal.cc:198] Starting premapped memory manager initialization...\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.039800 3508549 runtime_metric_service.cc:122] Successfully started Runtime Metric Service on port: 8431\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.039921 3508549 system.cc:1053] tpu::System initialized, current host id: 0, logical device ids: 0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.039952 3507724 tfrt_tpu_system_state.cc:213] CreateTpuSystemState: TPU initialization is successful and it took 1.111779055s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.039973 3507724 tfrt_tpu_system_state.cc:217] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.039997 3507724 tpu_host_allocator.cc:39] Premapped buffer is using alignment 512\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.040217 3507724 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.625925 3507724 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 21 instructions, modules: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.625952 3507724 autofdo_agent.cc:203] xla_tpu_autofdo_profile_dir updated to \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 20:55:48.625953 3507724 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 20:55:48\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 20:55:48.625953 3507724 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m running hlo passes for 416 instructions, modules: SyncTensorsGraph.424\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.625961 3507724 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.22 instructions: 21 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.626728 3507724 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.22 instructions: 142\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.626733 3507724 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.630831 3507724 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.632557 3507724 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 8.36749125ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.808436 3508709 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto  \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.815329 3508551 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 8.15236125ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.817583 3508551 2a886c8_compiler_base.cc:2647] final program bundle count: 457 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.820261 3508551 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822063 3508551 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (52.0K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822434 3508551 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822437 3508551 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 52.0K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822439 3508551 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 8.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822450 3508551 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.05M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822451 3508551 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822452 3508551 2a886c8_compiler_base.cc:3101]     program           52.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822452 3508551 2a886c8_compiler_base.cc:3101]     arguments            0B \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822453 3508551 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822453 3508551 2a886c8_compiler_base.cc:3101] Output size 20.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822454 3508551 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822462 3508551 2a886c8_compiler_base.cc:3105] Program sflag requirement 212B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822462 3508551 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822463 3508551 2a886c8_compiler_base.cc:3105]     scoped               8B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822463 3508551 2a886c8_compiler_base.cc:3105] Program vmem requirement 8.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822464 3508551 2a886c8_compiler_base.cc:3105]     scoped             8.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822464 3508551 2a886c8_compiler_base.cc:3105] Program smem requirement 592B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822465 3508551 2a886c8_compiler_base.cc:3105]     scoped             592B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822465 3508551 2a886c8_compiler_base.cc:3105] Program hbm requirement 52.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822466 3508551 2a886c8_compiler_base.cc:3105]     overlays          52.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822467 3508551 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (0 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822478 3508551 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 7.05054025ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822564 3508551 isa_program.cc:328] (HLO module SyncTensorsGraph.22): Executable fingerprint:c0190226e36366352ca22d7e2e9151366912c050fdf35b82d07ab1b113349f2d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822566 3508551 isa_program.cc:332] (HLO module SyncTensorsGraph.22): Executable fingerprint (including data segments):89cba99eca55ab4a5f0d76ec4b1e480653cb821a975fbc8b884f46e30673494a\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822567 3508551 isa_program.cc:335] (HLO module SyncTensorsGraph.22): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:55:48.822790 3507724 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 198.71265875ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.696296 3507724 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 416 instructions, modules: SyncTensorsGraph.424\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.696315 3507724 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.424 instructions: 416 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.698223 3507724 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.424 instructions: 456\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.698228 3507724 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.800787 3507724 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.856264 3507724 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 161.2185485ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.871632 3508716 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto  \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.670022 3508551 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 2.80745736725s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.898485 3508551 2a886c8_compiler_base.cc:2647] final program bundle count: 171,494 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.909901 3508551 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.965031 3508551 2a886c8_compiler_base.cc:2869] Program divided into 12 overlays without HLO functions (10.50M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063450 3508551 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.424\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063466 3508551 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 10.56M / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063476 3508551 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 62.60M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063490 3508551 2a886c8_compiler_base.cc:3101] Total hbm usage >= 316.43M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063492 3508551 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063493 3508551 2a886c8_compiler_base.cc:3101]     program          10.56M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063493 3508551 2a886c8_compiler_base.cc:3101]     arguments        47.87M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063494 3508551 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063494 3508551 2a886c8_compiler_base.cc:3101] Output size 16.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063495 3508551 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063507 3508551 2a886c8_compiler_base.cc:3105] Program sflag requirement 312B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063508 3508551 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063509 3508551 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063509 3508551 2a886c8_compiler_base.cc:3105]     HLO temp            92B (100.0% utilization: Unpadded (92B) Padded (92B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063510 3508551 2a886c8_compiler_base.cc:3105] Program vmem requirement 62.60M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063511 3508551 2a886c8_compiler_base.cc:3105]     scoped            9.60M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063511 3508551 2a886c8_compiler_base.cc:3105]     HLO temp         53.00M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (53.00M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063512 3508551 2a886c8_compiler_base.cc:3105] Program hbm requirement 10.56M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063512 3508551 2a886c8_compiler_base.cc:3105]     global            64.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063513 3508551 2a886c8_compiler_base.cc:3105]     overlays         10.50M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063514 3508551 2a886c8_compiler_base.cc:3105] Program smem requirement 832B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063514 3508551 2a886c8_compiler_base.cc:3105]     scoped             832B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063515 3508551 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 3.1K / 1.00M (103 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063548 3508551 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 393.4324135ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.071885 3508551 isa_program.cc:328] (HLO module SyncTensorsGraph.424): Executable fingerprint:46cf1139bf8135ecb3a69f9950a6e9ff304eac15a51b686ca68e4732d78da542\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.071891 3508551 isa_program.cc:332] (HLO module SyncTensorsGraph.424): Executable fingerprint (including data segments):e37e724e8049dba176ec1ef1bd986f69693f7fe9a8e45bb9368f26b316837067\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.071893 3508551 isa_program.cc:335] (HLO module SyncTensorsGraph\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.696315 3507724 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.424 instructions: 416 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.698223 3507724 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.424 instructions: 456\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.698228 3507724 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.800787 3507724 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.856264 3507724 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 161.2185485ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:15.871632 3508716 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugproto  \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.670022 3508551 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 2.80745736725s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.898485 3508551 2a886c8_compiler_base.cc:2647] final program bundle count: 171,494 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.909901 3508551 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:18.965031 3508551 2a886c8_compiler_base.cc:2869] Program divided into 12 overlays without HLO functions (10.50M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063450 3508551 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.424\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063466 3508551 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 10.56M / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063476 3508551 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 62.60M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063490 3508551 2a886c8_compiler_base.cc:3101] Total hbm usage >= 316.43M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063492 3508551 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063493 3508551 2a886c8_compiler_base.cc:3101]     program          10.56M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063493 3508551 2a886c8_compiler_base.cc:3101]     arguments        47.87M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063494 3508551 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063494 3508551 2a886c8_compiler_base.cc:3101] Output size 16.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063495 3508551 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063507 3508551 2a886c8_compiler_base.cc:3105] Program sflag requirement 312B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063508 3508551 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063509 3508551 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063509 3508551 2a886c8_compiler_base.cc:3105]     HLO temp            92B (100.0% utilization: Unpadded (92B) Padded (92B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063510 3508551 2a886c8_compiler_base.cc:3105] Program vmem requirement 62.60M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063511 3508551 2a886c8_compiler_base.cc:3105]     scoped            9.60M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063511 3508551 2a886c8_compiler_base.cc:3105]     HLO temp         53.00M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (53.00M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063512 3508551 2a886c8_compiler_base.cc:3105] Program hbm requirement 10.56M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063512 3508551 2a886c8_compiler_base.cc:3105]     global            64.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063513 3508551 2a886c8_compiler_base.cc:3105]     overlays         10.50M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063514 3508551 2a886c8_compiler_base.cc:3105] Program smem requirement 832B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063514 3508551 2a886c8_compiler_base.cc:3105]     scoped             832B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063515 3508551 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 3.1K / 1.00M (103 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.063548 3508551 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 393.4324135ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.071885 3508551 isa_program.cc:328] (HLO module SyncTensorsGraph.424): Executable fingerprint:46cf1139bf8135ecb3a69f9950a6e9ff304eac15a51b686ca68e4732d78da542\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.071891 3508551 isa_program.cc:332] (HLO module SyncTensorsGraph.424): Executable fingerprint (including data segments):e37e724e8049dba176ec1ef1bd986f69693f7fe9a8e45bb9368f26b316837067\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 20:57:19.071893 3508551 isa_program.cc:335] (HLO module SyncTensorsGraph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TpuResNetWorker pid=3539277, ip=10.138.0.2)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 21:09:32\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.541872 3539277 b295d63588a.cc:733] Linux version 6.5.0-1013-gcp (buildd@lcy02-amd64-064) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #13~22.04.1-Ubuntu SMP Wed Jan 24 23:39:40 UTC 2024\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553259 3539277 b295d63588a.cc:815] Process id 3539277\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553280 3539277 b295d63588a.cc:820] Current working directory /home/yifeit_google_com/resnet18\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553282 3539277 b295d63588a.cc:822] Current timezone is PDT (currently UTC -07:00)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553286 3539277 b295d63588a.cc:826] Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553287 3539277 b295d63588a.cc:827]  at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553288 3539277 b295d63588a.cc:828]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553289 3539277 b295d63588a.cc:829]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553290 3539277 b295d63588a.cc:832]  from changelist 719471581 with baseline 706826864 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/706826864.1/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553291 3539277 b295d63588a.cc:836] Build label: libtpu_lts_20241216_c_RC02\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553292 3539277 b295d63588a.cc:838] Build tool: Bazel, release r4rca-2024.12.08-1 (mainline @703883733)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553293 3539277 b295d63588a.cc:839] Build target: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553295 3539277 b295d63588a.cc:846] Command line arguments:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553296 3539277 b295d63588a.cc:848] argv[0]: './tpu_driver'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553298 3539277 b295d63588a.cc:848] argv[1]: '--minloglevel=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553300 3539277 b295d63588a.cc:848] argv[2]: '--stderrthreshold=3'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553301 3539277 b295d63588a.cc:848] argv[3]: '--v=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553303 3539277 b295d63588a.cc:848] argv[4]: '--vmodule='\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553304 3539277 b295d63588a.cc:848] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553305 3539277 b295d63588a.cc:848] argv[6]: '--max_log_size=1024'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553307 3539277 b295d63588a.cc:848] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553308 3539277 b295d63588a.cc:848] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553309 3539277 b295d63588a.cc:848] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553311 3539277 b295d63588a.cc:848] argv[10]: '--2a886c8_chips_per_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553312 3539277 b295d63588a.cc:848] argv[11]: '--2a886c8_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553313 3539277 b295d63588a.cc:848] argv[12]: '--2a886c8_hal_included_devs=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553315 3539277 b295d63588a.cc:848] argv[13]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553316 3539277 b295d63588a.cc:848] argv[14]: '--2a886c8_slice_builder_worker_addresses=10.138.0.2:8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553317 3539277 b295d63588a.cc:848] argv[15]: '--tpu_slice_builder_dump_chip=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553319 3539277 b295d63588a.cc:848] argv[16]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553320 3539277 b295d63588a.cc:848] argv[17]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553321 3539277 b295d63588a.cc:848] argv[18]: '--runtime_metric_service_port=8431'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553322 3539277 b295d63588a.cc:848] argv[19]: '--tpu_hbm_report_enable=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553324 3539277 b295d63588a.cc:848] argv[20]: '--tpu_hbm_report_frequency=5s'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553325 3539277 b295d63588a.cc:848] argv[21]: '--enable_runtime_uptime_telemetry=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553326 3539277 b295d63588a.cc:848] argv[22]: ''\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553328 3539277 b295d63588a.cc:848] argv[23]: '--xla_latency_hiding_scheduler_rerun=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553329 3539277 b295d63588a.cc:848] argv[24]: '--xla_tpu_prefer_async_allgather_to_allreduce=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553331 3539277 b295d63588a.cc:848] argv[25]: '--xla_tpu_enable_flash_attention=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553332 3539277 b295d63588a.cc:848] argv[26]: '--xla_enable_async_all_gather=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553333 3539277 b295d63588a.cc:848] argv[27]: '--xla_enable_async_collective_permute=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553687 3539277 init.cc:78] Remote crash gathering hook installed.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.553720 3539277 tpu_runtime_type_flags.cc:91] --tpu_use_tfrt not specified. Using default value: true\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.560286 3539277 tf_tpu_flags.cc:60] 2a886c8Platform is NOT registered.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.560485 3539277 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.560614 3539277 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.675814 3539277 config.cc:256] gRPC experiments enabled: max_pings_wo_data_throttle, monitoring_experiment, pick_first_new, time_caching_in_party, trace_record_callops, work_serializer_dispatch\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.685527 3539277 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 246, prefix = futex-default\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.685826 3539277 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <0A-00 00-00 00-00 00-00 E0-EA 9F-B9 44-56 00-00 80-A5 D9-59 C9-7F 00-00 58-95 41-5B C9-7F 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = (nil), clock = 0x5644b8cbd498\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.738335 3539277 singleton_tpu_states_manager.cc:72] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.738367 3539277 singleton_tpu_states_manager.cc:95] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.742090 3539277 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.742098 3539277 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.742103 3539277 tpu_version_flag.cc:53] Using auto-detected TPU version TPU v5 lite\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.745178 3539277 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.745183 3539277 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.748276 3539277 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.748281 3539277 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.751764 3539277 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.751773 3539277 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.756893 3540104 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.756917 3540104 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.756932 3540104 flags_util.cc:314] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.760064 3540104 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.760069 3540104 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:32.760075 3540104 tpu_network_factory.cc:62] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.110449 3540106 async_driver.cc:427] [/dev/vfio/1 tpu9:pe1:0] Driver opened.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.143968 3540104 slice_builder_helper.cc:98] Current host is used as SliceBuilder master.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.144286 3540104 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.157627 3540104 master.cc:221] Successfully initialized SliceBuilder master session 7fbe14bbda2b2392 with expected topology (1, 1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.157872 3540104 tpu_hal.cc:198] Starting premapped memory manager initialization...\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.954636 3540104 runtime_metric_service.cc:122] Successfully started Runtime Metric Service on port: 8431\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.954759 3540104 system.cc:1053] tpu::System initialized, current host id: 0, logical device ids: 0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.954790 3539277 tfrt_tpu_system_state.cc:213] CreateTpuSystemState: TPU initialization is successful and it took 1.20185207s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.954812 3539277 tfrt_tpu_system_state.cc:217] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.954839 3539277 tpu_host_allocator.cc:39] Premapped buffer is using alignment 512\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:33.955090 3539277 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:34.163916 3539277 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 21 instructions, modules: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:09:34.163945 3539277 autofdo_agent.cc:203] xla_tpu_autofdo_profile_dir updated to \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 21:09:34.163947 3539277 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 21:09:34\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 21:09:34.163947 3539277 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(autoscaler +37m14s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[33m(autoscaler +37m14s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 1.0, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[33m(autoscaler +37m49s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 1.0, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TpuResNetWorker pid=3547654, ip=10.138.0.2)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 21:12:32\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.301944 3547654 b295d63588a.cc:733] Linux version 6.5.0-1013-gcp (buildd@lcy02-amd64-064) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #13~22.04.1-Ubuntu SMP Wed Jan 24 23:39:40 UTC 2024\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312674 3547654 b295d63588a.cc:815] Process id 3547654\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312697 3547654 b295d63588a.cc:820] Current working directory /home/yifeit_google_com/resnet18\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312698 3547654 b295d63588a.cc:822] Current timezone is PDT (currently UTC -07:00)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312702 3547654 b295d63588a.cc:826] Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312703 3547654 b295d63588a.cc:827]  at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312704 3547654 b295d63588a.cc:828]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312705 3547654 b295d63588a.cc:829]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312706 3547654 b295d63588a.cc:832]  from changelist 719471581 with baseline 706826864 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/706826864.1/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312708 3547654 b295d63588a.cc:836] Build label: libtpu_lts_20241216_c_RC02\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312709 3547654 b295d63588a.cc:838] Build tool: Bazel, release r4rca-2024.12.08-1 (mainline @703883733)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312709 3547654 b295d63588a.cc:839] Build target: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312711 3547654 b295d63588a.cc:846] Command line arguments:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312711 3547654 b295d63588a.cc:848] argv[0]: './tpu_driver'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312714 3547654 b295d63588a.cc:848] argv[1]: '--minloglevel=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312716 3547654 b295d63588a.cc:848] argv[2]: '--stderrthreshold=3'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312717 3547654 b295d63588a.cc:848] argv[3]: '--v=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312718 3547654 b295d63588a.cc:848] argv[4]: '--vmodule='\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312719 3547654 b295d63588a.cc:848] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312721 3547654 b295d63588a.cc:848] argv[6]: '--max_log_size=1024'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312722 3547654 b295d63588a.cc:848] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312723 3547654 b295d63588a.cc:848] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312725 3547654 b295d63588a.cc:848] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312726 3547654 b295d63588a.cc:848] argv[10]: '--2a886c8_chips_per_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312727 3547654 b295d63588a.cc:848] argv[11]: '--2a886c8_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312729 3547654 b295d63588a.cc:848] argv[12]: '--2a886c8_hal_included_devs=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312730 3547654 b295d63588a.cc:848] argv[13]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312731 3547654 b295d63588a.cc:848] argv[14]: '--2a886c8_slice_builder_worker_addresses=10.138.0.2:8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312732 3547654 b295d63588a.cc:848] argv[15]: '--tpu_slice_builder_dump_chip=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312734 3547654 b295d63588a.cc:848] argv[16]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312735 3547654 b295d63588a.cc:848] argv[17]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312736 3547654 b295d63588a.cc:848] argv[18]: '--runtime_metric_service_port=8431'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312737 3547654 b295d63588a.cc:848] argv[19]: '--tpu_hbm_report_enable=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312739 3547654 b295d63588a.cc:848] argv[20]: '--tpu_hbm_report_frequency=5s'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312740 3547654 b295d63588a.cc:848] argv[21]: '--enable_runtime_uptime_telemetry=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312741 3547654 b295d63588a.cc:848] argv[22]: ''\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312743 3547654 b295d63588a.cc:848] argv[23]: '--xla_latency_hiding_scheduler_rerun=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312744 3547654 b295d63588a.cc:848] argv[24]: '--xla_tpu_prefer_async_allgather_to_allreduce=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312745 3547654 b295d63588a.cc:848] argv[25]: '--xla_tpu_enable_flash_attention=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312747 3547654 b295d63588a.cc:848] argv[26]: '--xla_enable_async_all_gather=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.312748 3547654 b295d63588a.cc:848] argv[27]: '--xla_enable_async_collective_permute=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.313127 3547654 init.cc:78] Remote crash gathering hook installed.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.313175 3547654 tpu_runtime_type_flags.cc:91] --tpu_use_tfrt not specified. Using default value: true\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.319942 3547654 tf_tpu_flags.cc:60] 2a886c8Platform is NOT registered.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.320162 3547654 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.320296 3547654 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.435908 3547654 config.cc:256] gRPC experiments enabled: max_pings_wo_data_throttle, monitoring_experiment, pick_first_new, time_caching_in_party, trace_record_callops, work_serializer_dispatch\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.445463 3547654 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 246, prefix = futex-default\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.445763 3547654 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <0A-00 00-00 00-00 00-00 00-C9 F6-55 D7-55 00-00 80-A5 B9-6E B4-7F 00-00 58-95 21-70 B4-7F 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = (nil), clock = 0x55d75255f4b0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.478786 3547654 singleton_tpu_states_manager.cc:72] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.478797 3547654 singleton_tpu_states_manager.cc:95] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.482362 3547654 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.482368 3547654 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.482373 3547654 tpu_version_flag.cc:53] Using auto-detected TPU version TPU v5 lite\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.485462 3547654 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.485467 3547654 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.488580 3547654 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.488585 3547654 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.491668 3547654 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.491673 3547654 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.496911 3548506 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.496932 3548506 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.496948 3548506 flags_util.cc:314] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.500068 3548506 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.500073 3548506 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.500079 3548506 tpu_network_factory.cc:62] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.856688 3548508 async_driver.cc:427] [/dev/vfio/0 tpu9:pe1:1] Driver opened.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.888106 3548506 slice_builder_helper.cc:98] Current host is used as SliceBuilder master.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.888411 3548506 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.901951 3548506 master.cc:221] Successfully initialized SliceBuilder master session d80f3f8a76eafa99 with expected topology (1, 1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:32.902223 3548506 tpu_hal.cc:198] Starting premapped memory manager initialization...\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.613121 3548506 runtime_metric_service.cc:122] Successfully started Runtime Metric Service on port: 8431\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.613251 3548506 system.cc:1053] tpu::System initialized, current host id: 0, logical device ids: 0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.613316 3547654 tfrt_tpu_system_state.cc:213] CreateTpuSystemState: TPU initialization is successful and it took 1.120516125s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.613329 3547654 tfrt_tpu_system_state.cc:217] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.613345 3547654 tpu_host_allocator.cc:39] Premapped buffer is using alignment 512\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.613494 3547654 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.824163 3547654 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 21 instructions, modules: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.824192 3547654 autofdo_agent.cc:203] xla_tpu_autofdo_profile_dir updated to \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 21:12:33.824194 3547654 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 21:12:33\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 21:12:33.824194 3547654 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.824202 3547654 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.22 instructions: 21 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.824942 3547654 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.22 instructions: 142\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.824948 3547654 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.828780 3547654 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.830491 3547654 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 8.102762ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.990835 3548665 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.997723 3548507 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 8.15913275ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:33.999818 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 457 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.002313 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.003997 3548507 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (52.0K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004256 3548507 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004259 3548507 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 52.0K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004261 3548507 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 8.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004270 3548507 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.05M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004272 3548507 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004272 3548507 2a886c8_compiler_base.cc:3101]     program           52.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004273 3548507 2a886c8_compiler_base.cc:3101]     arguments            0B \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004274 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004275 3548507 2a886c8_compiler_base.cc:3101] Output size 20.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004276 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004283 3548507 2a886c8_compiler_base.cc:3105] Program sflag requirement 212B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004284 3548507 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004285 3548507 2a886c8_compiler_base.cc:3105]     scoped               8B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004285 3548507 2a886c8_compiler_base.cc:3105] Program vmem requirement 8.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004286 3548507 2a886c8_compiler_base.cc:3105]     scoped             8.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004287 3548507 2a886c8_compiler_base.cc:3105] Program smem requirement 592B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004288 3548507 2a886c8_compiler_base.cc:3105]     scoped             592B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004288 3548507 2a886c8_compiler_base.cc:3105] Program hbm requirement 52.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004289 3548507 2a886c8_compiler_base.cc:3105]     overlays          52.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004290 3548507 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (0 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004298 3548507 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 6.4987505ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004381 3548507 isa_program.cc:328] (HLO module SyncTensorsGraph.22): Executable fingerprint:c0190226e36366352ca22d7e2e9151366912c050fdf35b82d07ab1b113349f2d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004383 3548507 isa_program.cc:332] (HLO module SyncTensorsGraph.22): Executable fingerprint (including data segments):89cba99eca55ab4a5f0d76ec4b1e480653cb821a975fbc8b884f46e30673494a\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004384 3548507 isa_program.cc:335] (HLO module SyncTensorsGraph.22): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:12:34.004595 3547654 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 182.32232325ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:09.816295 3547654 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 896 instructions, modules: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:09.816315 3547654 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.904 instructions: 896 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:09.823071 3547654 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.904 instructions: 2,697\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:09.823076 3547654 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:10.017440 3547654 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:10.119038 3547654 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 303.883792ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:10.146157 3548671 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:13.593101 3548507 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 3.46240149825s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:13.902680 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 233,911 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:13.917247 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:13.999611 3548507 2a886c8_compiler_base.cc:2869] Program divided into 14 overlays without HLO functions (14.31M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107144 3548507 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107158 3548507 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 18.17M / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107165 3548507 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 75.70M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107180 3548507 2a886c8_compiler_base.cc:3101] Total hbm usage >= 324.06M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107181 3548507 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107182 3548507 2a886c8_compiler_base.cc:3101]     program          18.17M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107182 3548507 2a886c8_compiler_base.cc:3101]     arguments        47.89M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107183 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107184 3548507 2a886c8_compiler_base.cc:3101] Output size 76.5K; shares 59.0K with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107184 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107204 3548507 2a886c8_compiler_base.cc:3105] Program sflag requirement 308B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107205 3548507 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107206 3548507 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107207 3548507 2a886c8_compiler_base.cc:3105]     HLO temp            88B (100.0% utilization: Unpadded (88B) Padded (88B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107208 3548507 2a886c8_compiler_base.cc:3105] Program hbm requirement 18.17M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107208 3548507 2a886c8_compiler_base.cc:3105]     global            64.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107209 3548507 2a886c8_compiler_base.cc:3105]     HLO temp          3.80M (99.2% utilization: Unpadded (120.5K) Padded (121.5K), 96.9% fragmentation (3.68M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107209 3548507 2a886c8_compiler_base.cc:3105]     overlays         14.31M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107210 3548507 2a886c8_compiler_base.cc:3105] Program vmem requirement 75.70M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107211 3548507 2a886c8_compiler_base.cc:3105]     scoped           10.68M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107211 3548507 2a886c8_compiler_base.cc:3105]     HLO temp         65.02M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (65.02M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107212 3548507 2a886c8_compiler_base.cc:3105] Program smem requirement 756B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107212 3548507 2a886c8_compiler_base.cc:3105]     scoped             756B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107213 3548507 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 3.1K / 1.00M (122 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.107340 3548507 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 514.10778875ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.118607 3548507 isa_program.cc:328] (HLO module SyncTensorsGraph.904): Executable fingerprint:cc691e50864fde1cbd4f235da9b0a90915ab728bd4670c8917c7fbe63e24836d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.118614 3548507 isa_program.cc:332] (HLO module SyncTensorsGraph.904): Executable fin\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m print (including data segments):567892d6a0af2238ec4b03a2e1379a977dc17108fd2f62086230a0a1452de712\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.118615 3548507 isa_program.cc:335] (HLO module SyncTensorsGraph.904): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.213748 3547654 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 4.399841414s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.708952 3547654 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 896 instructions, modules: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.708971 3547654 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.904 instructions: 896 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.716923 3547654 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.904 instructions: 2,698\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.716928 3547654 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.915245 3547654 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:31.027124 3547654 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 319.534147ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:31.055440 3551202 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.325945 3548507 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 3.2880063775s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.618432 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 234,094 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.632384 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.701392 3548507 2a886c8_compiler_base.cc:2869] Program divided into 14 overlays without HLO functions (14.32M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842082 3548507 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842094 3548507 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 18.58M / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842101 3548507 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 75.70M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842116 3548507 2a886c8_compiler_base.cc:3101] Total hbm usage >= 324.46M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842117 3548507 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842118 3548507 2a886c8_compiler_base.cc:3101]     program          18.58M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842118 3548507 2a886c8_compiler_base.cc:3101]     arguments        47.89M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842119 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842119 3548507 2a886c8_compiler_base.cc:3101] Output size 76.5K; shares 60.0K with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842120 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842143 3548507 2a886c8_compiler_base.cc:3105] Program sflag requirement 308B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842143 3548507 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842144 3548507 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842145 3548507 2a886c8_compiler_base.cc:3105]     HLO temp            88B (100.0% utilization: Unpadded (88B) Padded (88B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842145 3548507 2a886c8_compiler_base.cc:3105] Program hbm requirement 18.58M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842146 3548507 2a886c8_compiler_base.cc:3105]     global            66.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842146 3548507 2a886c8_compiler_base.cc:3105]     HLO temp          4.19M (90.0% utilization: Unpadded (120.6K) Padded (134.0K), 96.9% fragmentation (4.06M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842147 3548507 2a886c8_compiler_base.cc:3105]     overlays         14.32M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842148 3548507 2a886c8_compiler_base.cc:3105] Program vmem requirement 75.70M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842148 3548507 2a886c8_compiler_base.cc:3105]     scoped           10.68M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842149 3548507 2a886c8_compiler_base.cc:3105]     HLO temp         65.02M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (65.02M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842149 3548507 2a886c8_compiler_base.cc:3105] Program smem requirement 896B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842150 3548507 2a886c8_compiler_base.cc:3105]     scoped \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m gerprint (including data segments):567892d6a0af2238ec4b03a2e1379a977dc17108fd2f62086230a0a1452de712\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.118615 3548507 isa_program.cc:335] (HLO module SyncTensorsGraph.904): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:14.213748 3547654 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 4.399841414s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.708952 3547654 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 896 instructions, modules: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.708971 3547654 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.904 instructions: 896 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.716923 3547654 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.904 instructions: 2,698\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.716928 3547654 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:30.915245 3547654 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:31.027124 3547654 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 319.534147ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:31.055440 3551202 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly   \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.325945 3548507 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 3.2880063775s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.618432 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 234,094 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.632384 3548507 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.701392 3548507 2a886c8_compiler_base.cc:2869] Program divided into 14 overlays without HLO functions (14.32M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842082 3548507 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842094 3548507 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 18.58M / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842101 3548507 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 75.70M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842116 3548507 2a886c8_compiler_base.cc:3101] Total hbm usage >= 324.46M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842117 3548507 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842118 3548507 2a886c8_compiler_base.cc:3101]     program          18.58M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842118 3548507 2a886c8_compiler_base.cc:3101]     arguments        47.89M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842119 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842119 3548507 2a886c8_compiler_base.cc:3101] Output size 76.5K; shares 60.0K with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842120 3548507 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842143 3548507 2a886c8_compiler_base.cc:3105] Program sflag requirement 308B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842143 3548507 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842144 3548507 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842145 3548507 2a886c8_compiler_base.cc:3105]     HLO temp            88B (100.0% utilization: Unpadded (88B) Padded (88B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842145 3548507 2a886c8_compiler_base.cc:3105] Program hbm requirement 18.58M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842146 3548507 2a886c8_compiler_base.cc:3105]     global            66.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842146 3548507 2a886c8_compiler_base.cc:3105]     HLO temp          4.19M (90.0% utilization: Unpadded (120.6K) Padded (134.0K), 96.9% fragmentation (4.06M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842147 3548507 2a886c8_compiler_base.cc:3105]     overlays         14.32M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842148 3548507 2a886c8_compiler_base.cc:3105] Program vmem requirement 75.70M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842148 3548507 2a886c8_compiler_base.cc:3105]     scoped           10.68M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842149 3548507 2a886c8_compiler_base.cc:3105]     HLO temp         65.02M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (65.02M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842149 3548507 2a886c8_compiler_base.cc:3105] Program smem requirement 896B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:13:34.842150 3548507 2a886c8_compiler_base.cc:3105]     scoped \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(GpuResNetWorker pid=442611)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::GpuResNetWorker.__init__()\u001b[39m (pid=442611, ip=10.128.0.32, actor_id=46f8e961f7b2226c21de583303000000, repr=<__main__.GpuResNetWorker object at 0x7fd663e7c460>)\n",
      "\u001b[36m(GpuResNetWorker pid=442611)\u001b[0m   File \"/var/tmp/ipykernel_451716/3668095369.py\", line 78, in __init__\n",
      "\u001b[36m(GpuResNetWorker pid=442611)\u001b[0m AttributeError: 'GpuResNetWorker' object has no attribute 'device'\n",
      "\u001b[36m(TpuResNetWorker pid=3568758, ip=10.138.0.2)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 21:19:26\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.789265 3568758 b295d63588a.cc:733] Linux version 6.5.0-1013-gcp (buildd@lcy02-amd64-064) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #13~22.04.1-Ubuntu SMP Wed Jan 24 23:39:40 UTC 2024\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799480 3568758 b295d63588a.cc:815] Process id 3568758\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799502 3568758 b295d63588a.cc:820] Current working directory /home/yifeit_google_com/resnet18\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799504 3568758 b295d63588a.cc:822] Current timezone is PDT (currently UTC -07:00)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799506 3568758 b295d63588a.cc:826] Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799507 3568758 b295d63588a.cc:827]  at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799508 3568758 b295d63588a.cc:828]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799509 3568758 b295d63588a.cc:829]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799511 3568758 b295d63588a.cc:832]  from changelist 719471581 with baseline 706826864 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/706826864.1/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799512 3568758 b295d63588a.cc:836] Build label: libtpu_lts_20241216_c_RC02\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799513 3568758 b295d63588a.cc:838] Build tool: Bazel, release r4rca-2024.12.08-1 (mainline @703883733)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799514 3568758 b295d63588a.cc:839] Build target: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799516 3568758 b295d63588a.cc:846] Command line arguments:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799516 3568758 b295d63588a.cc:848] argv[0]: './tpu_driver'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799519 3568758 b295d63588a.cc:848] argv[1]: '--minloglevel=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799520 3568758 b295d63588a.cc:848] argv[2]: '--stderrthreshold=3'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799522 3568758 b295d63588a.cc:848] argv[3]: '--v=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799523 3568758 b295d63588a.cc:848] argv[4]: '--vmodule='\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799524 3568758 b295d63588a.cc:848] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799525 3568758 b295d63588a.cc:848] argv[6]: '--max_log_size=1024'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799527 3568758 b295d63588a.cc:848] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799528 3568758 b295d63588a.cc:848] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799529 3568758 b295d63588a.cc:848] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799531 3568758 b295d63588a.cc:848] argv[10]: '--2a886c8_chips_per_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799532 3568758 b295d63588a.cc:848] argv[11]: '--2a886c8_host_bounds=1,1,1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799533 3568758 b295d63588a.cc:848] argv[12]: '--2a886c8_hal_included_devs=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799535 3568758 b295d63588a.cc:848] argv[13]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799536 3568758 b295d63588a.cc:848] argv[14]: '--2a886c8_slice_builder_worker_addresses=10.138.0.2:8471'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799537 3568758 b295d63588a.cc:848] argv[15]: '--tpu_slice_builder_dump_chip=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799538 3568758 b295d63588a.cc:848] argv[16]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799540 3568758 b295d63588a.cc:848] argv[17]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799541 3568758 b295d63588a.cc:848] argv[18]: '--runtime_metric_service_port=8431'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799542 3568758 b295d63588a.cc:848] argv[19]: '--tpu_hbm_report_enable=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799544 3568758 b295d63588a.cc:848] argv[20]: '--tpu_hbm_report_frequency=5s'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799545 3568758 b295d63588a.cc:848] argv[21]: '--enable_runtime_uptime_telemetry=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799546 3568758 b295d63588a.cc:848] argv[22]: ''\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799547 3568758 b295d63588a.cc:848] argv[23]: '--xla_latency_hiding_scheduler_rerun=1'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799549 3568758 b295d63588a.cc:848] argv[24]: '--xla_tpu_prefer_async_allgather_to_allreduce=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799550 3568758 b295d63588a.cc:848] argv[25]: '--xla_tpu_enable_flash_attention=false'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799551 3568758 b295d63588a.cc:848] argv[26]: '--xla_enable_async_all_gather=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799552 3568758 b295d63588a.cc:848] argv[27]: '--xla_enable_async_collective_permute=true'\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799830 3568758 init.cc:78] Remote crash gathering hook installed.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.799860 3568758 tpu_runtime_type_flags.cc:91] --tpu_use_tfrt not specified. Using default value: true\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.806357 3568758 tf_tpu_flags.cc:60] 2a886c8Platform is NOT registered.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.806551 3568758 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.806692 3568758 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.907987 3568758 config.cc:256] gRPC experiments enabled: max_pings_wo_data_throttle, monitoring_experiment, pick_first_new, time_caching_in_party, trace_record_callops, work_serializer_dispatch\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.917679 3568758 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 246, prefix = futex-default\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.917967 3568758 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <0A-00 00-00 00-00 00-00 00-09 35-F8 8C-55 00-00 80-A5 99-8F E4-7E 00-00 58-95 01-91 E4-7E 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = (nil), clock = 0x558cf49f5478\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.974881 3568758 singleton_tpu_states_manager.cc:72] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.974891 3568758 singleton_tpu_states_manager.cc:95] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.978444 3568758 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.978450 3568758 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.978454 3568758 tpu_version_flag.cc:53] Using auto-detected TPU version TPU v5 lite\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.981539 3568758 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.981544 3568758 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.984626 3568758 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.984631 3568758 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.987744 3568758 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.987749 3568758 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.992502 3569617 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.992822 3569617 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.992836 3569617 flags_util.cc:314] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.995985 3569617 device_util.cc:112] Found 8 TPU v5 lite chips.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.995990 3569617 device_util.cc:114] After filtering devices, we have 1 chip(s) available.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:26.995996 3569617 tpu_network_factory.cc:62] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:27.356614 3569619 async_driver.cc:427] [/dev/vfio/1 tpu9:pe1:0] Driver opened.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:27.390490 3569617 slice_builder_helper.cc:98] Current host is used as SliceBuilder master.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:27.390832 3569617 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:27.406130 3569617 master.cc:221] Successfully initialized SliceBuilder master session 51608eee06d28c86 with expected topology (1, 1)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:27.406391 3569617 tpu_hal.cc:198] Starting premapped memory manager initialization...\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.241548 3569617 runtime_metric_service.cc:122] Successfully started Runtime Metric Service on port: 8431\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.241698 3569617 system.cc:1053] tpu::System initialized, current host id: 0, logical device ids: 0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.241727 3568758 tfrt_tpu_system_state.cc:213] CreateTpuSystemState: TPU initialization is successful and it took 1.252945414s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.241745 3568758 tfrt_tpu_system_state.cc:217] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.241770 3568758 tpu_host_allocator.cc:39] Premapped buffer is using alignment 512\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.241954 3568758 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.449166 3568758 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 21 instructions, modules: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.449195 3568758 autofdo_agent.cc:203] xla_tpu_autofdo_profile_dir updated to \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 21:19:28.449197 3568758 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log file created at: 2025/04/14 21:19:28\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Running on machine: t1v-n-8ed780ed-w-0\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built on Jan 24 2025 17:49:30 (1737769770)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built at cloud-tpus-runtime-release-tool@lmbco37.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Binary: Built for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m W0414 21:19:28.449197 3568758 autofdo_agent.cc:206] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.449206 3568758 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.22 instructions: 21 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.449984 3568758 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.22 instructions: 142\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.449990 3568758 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.453604 3568758 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.455175 3568758 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 7.87800625ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.627229 3569781 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly  \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.634120 3569618 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 8.2357865ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.636280 3569618 2a886c8_compiler_base.cc:2647] final program bundle count: 457 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.638832 3569618 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640503 3569618 2a886c8_compiler_base.cc:2869] Program divided into 2 overlays without HLO functions (52.0K).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640764 3569618 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.22\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640767 3569618 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 52.0K / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640770 3569618 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 8.0K / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640780 3569618 2a886c8_compiler_base.cc:3101] Total hbm usage >= 258.05M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640782 3569618 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640783 3569618 2a886c8_compiler_base.cc:3101]     program           52.0K \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640783 3569618 2a886c8_compiler_base.cc:3101]     arguments            0B \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640784 3569618 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640785 3569618 2a886c8_compiler_base.cc:3101] Output size 20.5K; shares 0B with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640786 3569618 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640794 3569618 2a886c8_compiler_base.cc:3105] Program sflag requirement 212B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640795 3569618 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640795 3569618 2a886c8_compiler_base.cc:3105]     scoped               8B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640796 3569618 2a886c8_compiler_base.cc:3105] Program vmem requirement 8.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640797 3569618 2a886c8_compiler_base.cc:3105]     scoped             8.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640798 3569618 2a886c8_compiler_base.cc:3105] Program smem requirement 592B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640798 3569618 2a886c8_compiler_base.cc:3105]     scoped             592B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640799 3569618 2a886c8_compiler_base.cc:3105] Program hbm requirement 52.0K:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640800 3569618 2a886c8_compiler_base.cc:3105]     overlays          52.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640801 3569618 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 2.4K / 1.00M (0 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640808 3569618 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 6.62292375ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640897 3569618 isa_program.cc:328] (HLO module SyncTensorsGraph.22): Executable fingerprint:c0190226e36366352ca22d7e2e9151366912c050fdf35b82d07ab1b113349f2d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640899 3569618 isa_program.cc:332] (HLO module SyncTensorsGraph.22): Executable fingerprint (including data segments):89cba99eca55ab4a5f0d76ec4b1e480653cb821a975fbc8b884f46e30673494a\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.640900 3569618 isa_program.cc:335] (HLO module SyncTensorsGraph.22): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:28.641125 3568758 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 193.952805ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:34.504570 3568758 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 896 instructions, modules: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:34.504589 3568758 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.904 inst\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m ructions: 896 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:34.511405 3568758 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.904 instructions: 2,697\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:34.511410 3568758 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:34.708128 3568758 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:34.808716 3568758 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 305.333992ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:34.832167 3569791 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly  \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.065455 3569618 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 3.24656515525s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.400513 3569618 2a886c8_compiler_base.cc:2647] final program bundle count: 233,911 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.415697 3569618 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.498652 3569618 2a886c8_compiler_base.cc:2869] Program divided into 14 overlays without HLO functions (14.31M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628904 3569618 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628919 3569618 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 18.17M / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628929 3569618 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 75.70M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628943 3569618 2a886c8_compiler_base.cc:3101] Total hbm usage >= 324.06M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628944 3569618 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628945 3569618 2a886c8_compiler_base.cc:3101]     program          18.17M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628945 3569618 2a886c8_compiler_base.cc:3101]     arguments        47.89M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628946 3569618 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628946 3569618 2a886c8_compiler_base.cc:3101] Output size 76.5K; shares 59.0K with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628947 3569618 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628969 3569618 2a886c8_compiler_base.cc:3105] Program sflag requirement 308B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628969 3569618 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628970 3569618 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628971 3569618 2a886c8_compiler_base.cc:3105]     HLO temp            88B (100.0% utilization: Unpadded (88B) Padded (88B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628971 3569618 2a886c8_compiler_base.cc:3105] Program hbm requirement 18.17M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628972 3569618 2a886c8_compiler_base.cc:3105]     global            64.5K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628972 3569618 2a886c8_compiler_base.cc:3105]     HLO temp          3.80M (99.2% utilization: Unpadded (120.5K) Padded (121.5K), 96.9% fragmentation (3.68M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628973 3569618 2a886c8_compiler_base.cc:3105]     overlays         14.31M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628974 3569618 2a886c8_compiler_base.cc:3105] Program vmem requirement 75.70M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628974 3569618 2a886c8_compiler_base.cc:3105]     scoped           10.68M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628975 3569618 2a886c8_compiler_base.cc:3105]     HLO temp         65.02M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (65.02M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628975 3569618 2a886c8_compiler_base.cc:3105] Program smem requirement 756B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628976 3569618 2a886c8_compiler_base.cc:3105]     scoped             756B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.628977 3569618 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage: 3.1K / 1.00M (122 parameters)\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.629001 3569618 2a886c8_compiler_base.cc:3135] CODE_GENERATION stage duration: 563.481678ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.640269 3569618 isa_program.cc:328] (HLO module SyncTensorsGraph.904): Executable fingerprint:cc691e50864fde1cbd4f235da9b0a90915ab728bd4670c8917c7fbe63e24836d\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.640276 3569618 isa_program.cc:332] (HLO module SyncTensorsGraph.904): Executable fingerprint (including data segments):567892d6a0af2238ec4b03a2e1379a977dc17108fd2f62086230a0a1452de712\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m             896B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494498 3569618 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m 19:38.640288 3569618 isa_program.cc:335] (HLO module SyncTensorsGraph.904): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:38.731207 3568758 2a886c8_compiler_base.cc:6679] END_TO_END stage duration: 4.22900549775s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:39.304989 3568758 2a886c8_compiler_base.cc:6104] XLA::TPU running hlo passes for 896 instructions, modules: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:39.305006 3568758 2a886c8_compiler_base.cc:6161] Initial HLO module: SyncTensorsGraph.904 instructions: 896 fingerprint: \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:39.313319 3568758 2a886c8_compiler_base.cc:6229] HLO optimizing module: SyncTensorsGraph.904 instructions: 2,698\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:39.313324 3568758 2a886c8_compiler_base.cc:6244] XLA::TPU HLO optimization\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:39.521721 3568758 2a886c8_compiler_base.cc:5347] XLA::TPU HLO PostOptimizationPipeline\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:39.627343 3568758 2a886c8_compiler_base.cc:6300] HLO_PASSES stage duration: 323.74063025ms\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:39.658414 3570385 memory_space_assignment_util.cc:234] Sliced prefetch options: go/debugonly  \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m max_slices: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m min_bytes: 1048576\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m all_slice_time_permutations_threshold: 4\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:42.927125 3569618 2a886c8_compiler_base.cc:9070] BACKEND_PASSES stage duration: 3.28579112575s\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.259892 3569618 2a886c8_compiler_base.cc:2647] final program bundle count: 234,094 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.275339 3569618 2a886c8_compiler_base.cc:2647] final program bundle count: 107 note this count does not reflect cycles spent executing delays.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.351459 3569618 2a886c8_compiler_base.cc:2869] Program divided into 14 overlays without HLO functions (14.32M).\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494416 3569618 2a886c8_compiler_base.cc:3038] XLA::TPU module name: SyncTensorsGraph.904\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494434 3569618 2a886c8_compiler_base.cc:3040] XLA::TPU program HBM usage: 18.58M / 15.75G\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494443 3569618 2a886c8_compiler_base.cc:3090] XLA::TPU program VMEM usage: 75.70M / 128.00M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494459 3569618 2a886c8_compiler_base.cc:3101] Total hbm usage >= 324.46M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494460 3569618 2a886c8_compiler_base.cc:3101]     reserved        258.00M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494461 3569618 2a886c8_compiler_base.cc:3101]     program          18.58M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494461 3569618 2a886c8_compiler_base.cc:3101]     arguments        47.89M \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494462 3569618 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494462 3569618 2a886c8_compiler_base.cc:3101] Output size 76.5K; shares 60.0K with arguments.\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494463 3569618 2a886c8_compiler_base.cc:3101] \n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494490 3569618 2a886c8_compiler_base.cc:3105] Program sflag requirement 308B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494491 3569618 2a886c8_compiler_base.cc:3105]     reserved           204B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494492 3569618 2a886c8_compiler_base.cc:3105]     scoped              16B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494492 3569618 2a886c8_compiler_base.cc:3105]     HLO temp            88B (100.0% utilization: Unpadded (88B) Padded (88B), 0.0% fragmentation (0B))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494493 3569618 2a886c8_compiler_base.cc:3105] Program hbm requirement 18.58M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494493 3569618 2a886c8_compiler_base.cc:3105]     global            66.0K\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494494 3569618 2a886c8_compiler_base.cc:3105]     HLO temp          4.19M (90.0% utilization: Unpadded (120.6K) Padded (134.0K), 96.9% fragmentation (4.06M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494495 3569618 2a886c8_compiler_base.cc:3105]     overlays         14.32M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494495 3569618 2a886c8_compiler_base.cc:3105] Program vmem requirement 75.70M:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494496 3569618 2a886c8_compiler_base.cc:3105]     scoped           10.68M\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494496 3569618 2a886c8_compiler_base.cc:3105]     HLO temp         65.02M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (65.02M))\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494497 3569618 2a886c8_compiler_base.cc:3105] Program smem requirement 896B:\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494497 3569618 2a886c8_compiler_base.cc:3105]     scoped             896B\n",
      "\u001b[36m(pid=, ip=10.138.0.2)\u001b[0m I0414 21:19:43.494498 3569618 2a886c8_compiler_base.cc:3113] XLA::TPU program SMEM usage\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1e4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision import datasets, models, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93709d1f02db4e66ba407f97b8d83972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b88baf1b03492d80ea7014796dae83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b742473e2544cf999b1b1c95e713248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741753a3a3604bc995d255b9f0b5c9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/19 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2302ca275b14b83a2cefac50767c062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00019.parquet:   0%|          | 0.00/500M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b01aeba2af4eb29acfe6aef3df45d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00019.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4837c31a234c97b3c51bf8081b8fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00019.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395a7c54536e42299366bee4f3110bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00019.parquet:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aee1cd7619949d2b9d98c7882dc5dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00004-of-00019.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbf737f20d24440a2dc1bfdf33cf1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00005-of-00019.parquet:   0%|          | 0.00/503M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6eed51debd42f0bbd052f9a2208db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00006-of-00019.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6303eee0e182443dbe4c6d0647679067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00007-of-00019.parquet:   0%|          | 0.00/493M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8260ae14424e7a8bf7ed10338e9162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00008-of-00019.parquet:   0%|          | 0.00/497M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c383637249d44d489d8047f14a2d8909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00009-of-00019.parquet:   0%|          | 0.00/503M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdca46ae562d48d3a232d76fafd60ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00010-of-00019.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdfe631df644fd3a6f9eeab44d0fc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00011-of-00019.parquet:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dd7eb31d5b4af9a29f7c9b8398b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00012-of-00019.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5bdcb156e74b398481d8d6f42a6067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00013-of-00019.parquet:   0%|          | 0.00/504M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afd9949c9604fbd85b6e10ce4915aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00014-of-00019.parquet:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e028efe4e444d6ad5ba1a36c802faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00015-of-00019.parquet:   0%|          | 0.00/489M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883ce80ae28547a6bb7e1c583bf57bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00016-of-00019.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a03359fcc14f3ca1939b660d3df9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00017-of-00019.parquet:   0%|          | 0.00/489M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ae313ebe5a4fcc9377b9bbd299a36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00018-of-00019.parquet:   0%|          | 0.00/489M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a7d965c3cf412990c43b99c95c78d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid-00000-of-00003.parquet:   0%|          | 0.00/388M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f2b9cf1d98499fa165b070ea165af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid-00001-of-00003.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8f6db6ecbd4687902f043bd8cd9f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid-00002-of-00003.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55ea630d92a4fc39d82fc994cc6fbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00003.parquet:   0%|          | 0.00/391M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0ab6a802ea42e098360e8f7a640e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00001-of-00003.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a646f6d21d466baf6b736b4fb9c259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00002-of-00003.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c58ff00c4ce4b3e94f80762c664799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/162770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b527d6d6ee324719b232bc8999ae5312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/19867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7763a108540e4abe8854d8a7f1e5bba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/19962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download dataset\n",
    "train_dataset = load_dataset(\"flwrlabs/celeba\", split=\"test\", trust_remote_code=True)\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "CLASSES = train_dataset.unique('celeb_id')  # type: ignore\n",
    "NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410eea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes:\", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44958157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ff0d5bc4b6434b9c4d4a150ce13d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/19962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset loader...\n"
     ]
    }
   ],
   "source": [
    "def transform_example(ex):\n",
    "    assert isinstance(CLASSES, list)\n",
    "    imgs = ex[\"image\"]\n",
    "    labels = ex[\"celeb_id\"]\n",
    "    transform_fct = v2.Compose([\n",
    "        v2.PILToTensor(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Resize(size=(224, 224)),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_bbox_pairs = [transform_fct(img, label) for img, label in zip(imgs, labels)]\n",
    "\n",
    "    return {\"image\": [img for img, label in img_bbox_pairs],\n",
    "            \"label\": [CLASSES.index(label) for img, label in img_bbox_pairs]}\n",
    "\n",
    "train_dataset = train_dataset.map(transform_example, num_proc=4, batched=True, writer_batch_size=4_000, cache_file_name=\"cache/celeba.pt\")  # type: ignore\n",
    "\n",
    "print(\"Preparing dataset loader...\")\n",
    "# Create a data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, num_workers=0, shuffle=False)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a63a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class BaseActor:\n",
    "  \n",
    "  def __init__(self, logger, model, device):\n",
    "    self.logger = logger\n",
    "    self.model = model\n",
    "    self.device = device\n",
    "  \n",
    "  def reset_classifier(\n",
    "      self,\n",
    "      fc_state_dict: dict[str, Any],\n",
    "      conv1_state_dict: dict[str, Any],\n",
    "      bn1_state_dict: dict[str, Any]\n",
    "    ) -> bool:\n",
    "    \"\"\"Replaces the weights of fc, conv1, and bn1 layers.\"\"\"\n",
    "    logger = self.logger\n",
    "    logger.info(\"Resetting classifier layers...\")\n",
    "    try:\n",
    "      # Ensure layers exist before loading state dict\n",
    "      if hasattr(self.model, 'fc') and self.model.fc is not None:\n",
    "          self.model.fc.load_state_dict(fc_state_dict)\n",
    "          logger.info(\"Loaded state dict into fc layer.\")\n",
    "      else:\n",
    "          logger.warning(\"fc layer not found or is None in the model.\")\n",
    "\n",
    "      if hasattr(self.model, 'conv1') and self.model.conv1 is not None:\n",
    "          self.model.conv1.load_state_dict(conv1_state_dict)\n",
    "          logger.info(\"Loaded state dict into conv1 layer.\")\n",
    "      else:\n",
    "          logger.warning(\"conv1 layer not found or is None in the model.\")\n",
    "\n",
    "      if hasattr(self.model, 'bn1') and self.model.bn1 is not None:\n",
    "          self.model.bn1.load_state_dict(bn1_state_dict)\n",
    "          logger.info(\"Loaded state dict into bn1 layer.\")\n",
    "      else:\n",
    "          logger.warning(\"bn1 layer not found or is None in the model.\")\n",
    "\n",
    "      self.model.to(self.device)\n",
    "      logger.info(\"Classifier layers reset successfully.\")\n",
    "      return True\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Failed to reset classifier layers: {e}\", exc_info=True)\n",
    "      return False\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Actor for GPU Execution\n",
    "# ==========================\n",
    "@ray.remote(num_gpus=1)\n",
    "class GpuResNetWorker(BaseActor):\n",
    "  def __init__(self, num_classes: int):\n",
    "    import torch\n",
    "\n",
    "    self.pid = os.getpid()\n",
    "    logger = logging.getLogger(f\"GpuActor-{self.pid}\")\n",
    "    logger.info(f\"Initializing GpuResNetWorker (PID: {self.pid})\")\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "      logger.error(\"CUDA not available on this worker!\")\n",
    "      raise RuntimeError(\"CUDA not available for GpuResNetWorker\")\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    logger.info(f\"Using device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "    # Load a pretrained ResNet18 model\n",
    "    logger.info(\"Loading pretrained ResNet18 model...\")\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) # Use updated API\n",
    "    self.num_ftrs = model.fc.in_features\n",
    "\n",
    "    logger.info(\"Moving model to GPU device...\")\n",
    "    # Move the model to the GPU\n",
    "    model = model.to(device)\n",
    "    logger.info(\"Model loaded and moved to GPU successfully.\")\n",
    "\n",
    "    super().__init__(logger, model, device)\n",
    "\n",
    "  def get_num_features(self) -> int:\n",
    "    \"\"\"Returns the number of features in the model.\"\"\"\n",
    "    return self.num_ftrs\n",
    "\n",
    "  def get_device_info(self) -> str:\n",
    "    return f\"GPU Actor (PID: {self.pid}) using {self.device} ({torch.cuda.get_device_name(0)})\"\n",
    "\n",
    "  def run_forward_pass(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Runs a forward pass using the resident model.\"\"\"\n",
    "    self.logger.info(f\"Received tensor of shape: {input_tensor.shape}\")\n",
    "    input_tensor_gpu = input_tensor.to(self.device)\n",
    "    with torch.no_grad():\n",
    "      output = self.model(input_tensor_gpu)\n",
    "    self.logger.info(f\"Forward pass complete. Output shape: {output.shape}\")\n",
    "    # Return tensor on the CPU for easier handling by the caller\n",
    "    return output.cpu()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Actor for TPU Execution\n",
    "# ==========================\n",
    "@ray.remote(resources={\"TPU\": 1})\n",
    "class TpuResNetWorker(BaseActor):\n",
    "  def __init__(self, num_classes: int):\n",
    "    self.pid = os.getpid()\n",
    "    logger = logging.getLogger(f\"TpuActor-{self.pid}\")\n",
    "    logger.info(f\"Initializing TpuResNetWorker (PID: {self.pid})\")\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    try:\n",
    "      import torch_xla\n",
    "      import torch_xla.core.xla_model as xm\n",
    "      logger.info(\"torch_xla imported successfully.\")\n",
    "    except ImportError:\n",
    "      logger.error(\"torch_xla not found! Cannot use TPU.\")\n",
    "      raise ImportError(\"torch_xla is required for TpuResNetWorker\")\n",
    "\n",
    "    device = torch_xla.device()\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "    # Load the *same* pretrained ResNet18 model structure\n",
    "    logger.info(\"Loading pretrained ResNet18 model...\")\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) # Use updated API\n",
    "    self.num_ftrs = model.fc.in_features\n",
    "\n",
    "    logger.info(\"Moving model to TPU device...\")\n",
    "    # Move the model to the TPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    torch_xla.sync()\n",
    "    logger.info(\"Model loaded and moved to TPU successfully.\")\n",
    "\n",
    "    super().__init__(logger, model, device)\n",
    "  \n",
    "  def get_num_features(self) -> int:\n",
    "    \"\"\"Returns the number of features in the model.\"\"\"\n",
    "    return self.num_ftrs\n",
    "\n",
    "  def get_device_info(self) -> str:\n",
    "    import torch_xla\n",
    "    return f\"TPU Actor (PID: {self.pid}) using {self.device} ({torch_xla.tpu.get_tpu_env()['ACCELERATOR_TYPE']})\"\n",
    "\n",
    "  def run_forward_pass(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Runs a forward pass using the resident model.\"\"\"\n",
    "    import torch_xla\n",
    "\n",
    "    self.logger.info(f\"Received tensor of shape: {input_tensor.shape}\")\n",
    "    input_tensor_tpu = input_tensor.to(self.device)\n",
    "    with torch.no_grad():\n",
    "      output = self.model(input_tensor_tpu)\n",
    "\n",
    "    torch_xla.sync()\n",
    "    self.logger.info(f\"Forward pass complete. Output shape: {output.shape}\")\n",
    "    return output.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0219288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Actor (PID: 442614) using cuda (NVIDIA A100-SXM4-80GB)\n"
     ]
    }
   ],
   "source": [
    "gpu_actor = GpuResNetWorker.remote(num_classes=NUM_CLASSES)  # type: ignore\n",
    "gpu_info = ray.get(gpu_actor.get_device_info.remote())\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35a44fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU Actor (PID: 3568758) using xla:0 (v5litepod-8)\n"
     ]
    }
   ],
   "source": [
    "tpu_actor = TpuResNetWorker.remote(num_classes=NUM_CLASSES)  # type: ignore\n",
    "tpu_info = ray.get(tpu_actor.get_device_info.remote())\n",
    "print(tpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31015604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the classifier layers in both actors to be the same\n",
    "import torch.nn as nn\n",
    "num_ftrs: int = ray.get(gpu_actor.get_num_features.remote())\n",
    "assert num_ftrs == ray.get(tpu_actor.get_num_features.remote()), \"Number of features mismatch between GPU and TPU actors.\"\n",
    "\n",
    "fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "assert ray.get(gpu_actor.reset_classifier.remote(\n",
    "    fc.state_dict(),\n",
    "    conv1.state_dict(),\n",
    "    bn1.state_dict()\n",
    "  ))\n",
    "assert ray.get(tpu_actor.reset_classifier.remote(\n",
    "    fc.state_dict(),\n",
    "    conv1.state_dict(),\n",
    "    bn1.state_dict()\n",
    "  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0797f4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run\n",
      "Max absolute difference between GPU and TPU outputs: 0.032161\n",
      "Max absolute difference between GPU and TPU outputs: 0.039519\n",
      "Max absolute difference between GPU and TPU outputs: 0.033644\n",
      "Max absolute difference between GPU and TPU outputs: 0.028708\n",
      "Max absolute difference between GPU and TPU outputs: 0.038301\n",
      "Max absolute difference between GPU and TPU outputs: 0.033892\n",
      "Max absolute difference between GPU and TPU outputs: 0.035464\n",
      "Max absolute difference between GPU and TPU outputs: 0.033167\n",
      "Max absolute difference between GPU and TPU outputs: 0.036825\n",
      "Max absolute difference between GPU and TPU outputs: 0.038834\n",
      "Max absolute difference between GPU and TPU outputs: 0.038446\n",
      "Max absolute difference between GPU and TPU outputs: 0.038007\n",
      "Max absolute difference between GPU and TPU outputs: 0.035501\n",
      "Max absolute difference between GPU and TPU outputs: 0.034619\n",
      "Max absolute difference between GPU and TPU outputs: 0.032978\n",
      "Max absolute difference between GPU and TPU outputs: 0.031172\n",
      "Max absolute difference between GPU and TPU outputs: 0.035390\n",
      "Max absolute difference between GPU and TPU outputs: 0.037134\n",
      "Max absolute difference between GPU and TPU outputs: 0.030368\n",
      "Max absolute difference between GPU and TPU outputs: 0.035395\n",
      "Absolute difference between GPU and TPU outputs:\n",
      "\n",
      "Max: 0.039519\n",
      "Min: 0.000000\n",
      "Mean: 0.007037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test run\")\n",
    "deltas = []\n",
    "\n",
    "for i in range(20):\n",
    "  dummy_input = torch.randn(4, 3, 224, 224)\n",
    "  input_ref = ray.put(dummy_input)\n",
    "  gpu_future = gpu_actor.run_forward_pass.remote(input_ref)\n",
    "  tpu_future = tpu_actor.run_forward_pass.remote(input_ref)\n",
    "  gpu_output: torch.Tensor = ray.get(gpu_future)\n",
    "  tpu_output: torch.Tensor = ray.get(tpu_future)\n",
    "\n",
    "  assert gpu_output.shape == (4, 1000)\n",
    "  if gpu_output.shape == tpu_output.shape:\n",
    "    abs_diff = torch.abs(gpu_output - tpu_output)\n",
    "    print(f\"Max absolute difference between GPU and TPU outputs: {abs_diff.max().item():.6f}\")\n",
    "  else:\n",
    "    raise ValueError(\"Output shapes differ between GPU and TPU!\")\n",
    "  deltas.append(abs_diff)\n",
    "\n",
    "print(f\"\"\"Absolute difference between GPU and TPU outputs:\n",
    "\n",
    "Max: {np.max(deltas):.6f}\n",
    "Min: {np.min(deltas):.6f}\n",
    "Mean: {np.mean(deltas):.6f}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ff232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
